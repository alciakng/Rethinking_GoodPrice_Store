import json
import geojson
from matplotlib import cm
import pandas as pd
import geopandas as gpd
import plotly.express as px
import plotly.graph_objects as go
import numpy as np
import requests                      
import Levenshtein
import pydeck as pdk

from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import MinMaxScaler
from sklearn.discriminant_analysis import StandardScaler
from sklearn.metrics import mean_squared_log_error
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
from sklearn.cross_decomposition import PLSRegression

from tqdm import tqdm
from concurrent.futures import ThreadPoolExecutor, as_completed
from linearmodels.panel import PanelOLS
from statsmodels.stats.outliers_influence import variance_inflation_factor
from statsmodels.tools.tools import add_constant

from mapboxgl.viz import ChoroplethViz
from mapboxgl.utils import df_to_geojson
from mapboxgl.utils import create_color_stops
from mapboxgl.utils import create_numeric_stops
from scipy.cluster.hierarchy import linkage, dendrogram, fcluster
from scipy.stats import zscore
from scipy.stats import skew

import seaborn as sns
import matplotlib as mpl
import matplotlib.pyplot as plt
import matplotlib.cm as cm
import matplotlib.colors as mcolors

from stargazer.stargazer import Stargazer
from util.common_util import load_clustered_geodataframe


# =======================================
# 0. ê³µí†µí•¨ìˆ˜_ì •ì˜
# - zscore_scale 
# - check_variable_skewness (ì™œë„íŒŒì•…)
# - check_outliers_std (ì´ìƒì¹˜íŒŒì•…)
# =======================================
 
# ë¡œê·¸ë³€í™˜ í•¨ìˆ˜
def apply_log_transform(df, columns):
    """
    ë¡œê·¸ ë˜ëŠ” log1p ë³€í™˜ì„ ìˆ˜í–‰í•˜ê³ , ì›ë³¸ ì»¬ëŸ¼ëª… ë’¤ì— '_log'ë¥¼ ë¶™ì—¬ ìƒˆë¡œìš´ ì»¬ëŸ¼ ìƒì„±
    :param df: ì›ë³¸ DataFrame
    :param columns: ë¡œê·¸ ë³€í™˜í•  ì»¬ëŸ¼ ë¦¬ìŠ¤íŠ¸
    :return: ë³€í™˜ëœ DataFrame (in-place ì•„ë‹˜)
    """
    df_transformed = df.copy()
    
    for col in columns:
        if (df_transformed[col] <= 0).any():
            print(col + " â†’ log1p ì ìš©")
            df_transformed[f'{col}_log'] = np.log1p(df_transformed[col])
        else:
            print(col + " â†’ log ì ìš©")
            df_transformed[f'{col}_log'] = np.log(df_transformed[col])
    
    return df_transformed


# zscore_scale
def apply_zscore_scaling(df, columns):
    """
    ì§€ì •ëœ ì»¬ëŸ¼ì— ëŒ€í•´ Z-score ìŠ¤ì¼€ì¼ë§ì„ ìˆ˜í–‰
    :param df: ì›ë³¸ DataFrame
    :param columns: ìŠ¤ì¼€ì¼ë§í•  ì»¬ëŸ¼ ë¦¬ìŠ¤íŠ¸
    :return: ìŠ¤ì¼€ì¼ë§ëœ DataFrame (in-place ì•„ë‹˜)
    """
    df_scaled = df.copy()
    
    for col in columns:
        print(col + " â†’ zscore scaling")
        df_scaled[col] = zscore(df_scaled[col])
    
    return df_scaled

mpl.rc('font', family='AppleGothic') # í•œê¸€ê¹¨ì§ ë¬¸ì œ
def check_variable_skewness(df, threshold=1.0):
    numeric_cols = df.select_dtypes(include=[np.number]).columns
    skew_info = {}

    for col in numeric_cols:
        col_skew = skew(df[col].dropna())
        skew_info[col] = col_skew

    skew_df = pd.DataFrame.from_dict(skew_info, orient='index', columns=['Skewness'])
    skew_df = skew_df.sort_values('Skewness', ascending=False)

    # ê¸°ì¤€ì„  í‘œì‹œ
    def skew_label(value):
        if abs(value) < 0.5:
            return 'âˆ¼ ëŒ€ì¹­'
        elif abs(value) < threshold:
            return 'ì•½ê°„ ì™œë„'
        else:
            return 'ê°•í•œ ì™œë„'

    skew_df['í•´ì„'] = skew_df['Skewness'].apply(skew_label)

    print("ğŸ“Š ë³€ìˆ˜ë³„ ì™œë„ ìš”ì•½:")
    print(skew_df)

    # íˆìŠ¤í† ê·¸ë¨ ì‹œê°í™”
    fig, axs = plt.subplots(nrows=int(np.ceil(len(numeric_cols)/3)), ncols=3, figsize=(16, int(len(numeric_cols)*1.5)))
    axs = axs.flatten()

    for i, col in enumerate(numeric_cols):
        sns.histplot(df[col].dropna(), kde=True, ax=axs[i])
        axs[i].set_title(f"{col} (Skew: {skew_info[col]:.2f})")

    plt.tight_layout()
    plt.show()

    return skew_df

def check_outliers_std(df, threshold=3.0):
    numeric_cols = df.select_dtypes(include=[np.number]).columns
    outlier_info = {}

    for col in numeric_cols:
        data = df[col].dropna()
        mean = data.mean()
        std = data.std()
        lower_bound = mean - threshold * std
        upper_bound = mean + threshold * std

        outliers = data[(data < lower_bound) | (data > upper_bound)]
        outlier_ratio = len(outliers) / len(data)

        outlier_info[col] = {
            'Outliers': len(outliers),
            'Outlier_Ratio': outlier_ratio,
            'Mean': mean,
            'Std': std,
            'Lower_Bound': lower_bound,
            'Upper_Bound': upper_bound
        }

    # ìš”ì•½í‘œ ìƒì„±
    outlier_df = pd.DataFrame(outlier_info).T
    outlier_df = outlier_df.sort_values('Outlier_Ratio', ascending=False)

    print("ğŸ“ 3Ïƒ ê¸°ì¤€ ì´ìƒì¹˜ íƒì§€ ê²°ê³¼:")
    print(outlier_df[['Outliers', 'Outlier_Ratio']])

    # ì‹œê°í™”
    fig, axs = plt.subplots(nrows=int(np.ceil(len(numeric_cols)/3)), ncols=3, figsize=(16, int(len(numeric_cols)*1.5)))
    axs = axs.flatten()

    for i, col in enumerate(numeric_cols):
        sns.histplot(df[col].dropna(), kde=True, ax=axs[i], color='lightcoral')
        axs[i].axvline(outlier_info[col]['Lower_Bound'], color='blue', linestyle='--', label='Lower Bound')
        axs[i].axvline(outlier_info[col]['Upper_Bound'], color='blue', linestyle='--', label='Upper Bound')
        axs[i].set_title(f"{col} (Outliers: {outlier_info[col]['Outliers']})")
        axs[i].legend()

    plt.tight_layout()
    plt.show()

    return outlier_df

# ì´ìƒì¹˜ì œê±° í•¨ìˆ˜ 
def drop_outlier_rows_std(df, cols, threshold=3.0):
    df_cleaned = df.copy()
    valid_cols = []

    # ìœ íš¨í•œ ìˆ˜ì¹˜í˜• ì»¬ëŸ¼ë§Œ í•„í„°ë§
    for col in cols:
        if col in df.columns and np.issubdtype(df[col].dtype, np.number):
            valid_cols.append(col)
        else:
            print(f"âš ï¸ ì»¬ëŸ¼ '{col}'ì€ ì¡´ì¬í•˜ì§€ ì•Šê±°ë‚˜ ìˆ˜ì¹˜í˜•ì´ ì•„ë‹™ë‹ˆë‹¤. ì œì™¸ë©ë‹ˆë‹¤.")

    if not valid_cols:
        print("âŒ ì´ìƒì¹˜ ê²€ì¶œí•  ìˆ˜ ìˆëŠ” ìˆ˜ì¹˜í˜• ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        return df_cleaned

    # ì´ìƒì¹˜ ë§ˆìŠ¤í¬ ìƒì„±
    outlier_mask = pd.DataFrame(False, index=df.index, columns=valid_cols)

    for col in valid_cols:
        data = df[col]
        mean = data.mean()
        std = data.std()
        lower_bound = mean - threshold * std
        upper_bound = mean + threshold * std

        outlier_mask[col] = (data < lower_bound) | (data > upper_bound)

    # ì´ìƒì¹˜ í¬í•¨ëœ í–‰ ì‹ë³„
    rows_with_outliers = outlier_mask.any(axis=1)
    num_outliers = rows_with_outliers.sum()

    print(f"ğŸ§¹ ì´ìƒì¹˜ í¬í•¨ í–‰ ì œê±°: {num_outliers}ê°œ í–‰ ì‚­ì œë¨")

    # ì¸ë±ìŠ¤ ìœ ì§€í•œ ì±„ ì´ìƒì¹˜ í–‰ ì œê±° (reset_index ì œê±°)
    df_no_outliers = df_cleaned[~rows_with_outliers]

    return df_no_outliers

# rmsle ê³„ì‚°í•¨ìˆ˜ 
def compute_rmsle_from_result(result, df):
    """
    PanelOLS íšŒê·€ ê²°ê³¼ì—ì„œ RMSLEë¥¼ ê³„ì‚°í•˜ëŠ” í•¨ìˆ˜
    
    Parameters:
    -----------
    result : linearmodels.panel.results.PanelEffectsResults
        PanelOLSì˜ íšŒê·€ ê²°ê³¼ ê°ì²´ (ex: result = model.fit())
    
    df : pd.DataFrame
        ì›ë³¸ ë°ì´í„°í”„ë ˆì„ (fitted_valuesì˜ ì¸ë±ìŠ¤ì™€ ë§ì•„ì•¼ í•¨)
    
    Returns:
    --------
    rmsle : float
        ë¡œê·¸ ì—­ë³€í™˜ í›„ ì˜ˆì¸¡ê°’ê³¼ ì‹¤ì œê°’ ê°„ RMSLE
    """
    # ì¢…ì†ë³€ìˆ˜ëª… ì¶”ì¶œ
    y_var = result.model.dependent.vars[0]

    # ì˜ˆì¸¡ê°’ (log scale)
    y_pred_log = result.fitted_values

    # ì‹¤ì œê°’ (log scale)
    y_true_log = df.loc[y_pred_log.index, y_var]

    # ë¡œê·¸ ì—­ë³€í™˜
    y_pred_actual = np.expm1(y_pred_log)
    y_true_actual = np.expm1(y_true_log)

    # ìŒìˆ˜ ë°©ì§€ (RMSLEëŠ” ìŒìˆ˜ ì…ë ¥ ë¶ˆê°€)
    y_pred_fixed = np.clip(y_pred_actual, 0, None)
    y_true_fixed = np.clip(y_true_actual, 0, None)

    # RMSLE ê³„ì‚°
    rmsle = np.sqrt(mean_squared_log_error(y_true_fixed, y_pred_fixed))
    return rmsle


def save_full_model_output(results, rmsle=None, filename="full_model_output.csv"):
    # --- ìš”ì•½ í†µê³„ ---
    n_obs = results.nobs   
    n_periods = getattr(result, 'time_info', {}).get('total', None) 
    k = len(results.params)
    r2 = results.rsquared
    adj_r2 = 1 - (1 - r2) * (n_obs - 1) / (n_obs - k - 1)

    summary_dict = {
        "No. of Observations": n_obs,
        "No. of Time Periods": n_periods,
        "R-squared (Overall)": round(r2, 4),
        "Adjusted R-squared": round(adj_r2, 4),
        "R-squared (Within)": round(results.rsquared_within, 4),
        "R-squared (Between)": round(results.rsquared_between, 4),
        "Log-likelihood": round(results.loglik, 2),
        "F-statistic": round(results.f_statistic.stat, 2),
        "F-statistic (p-value)": round(results.f_statistic.pval, 4),
        "RMSLE": round(rmsle, 4) if rmsle is not None else "N/A"
    }

    df_summary = pd.DataFrame(summary_dict.items(), columns=["Metric", "Value"])

    # --- ê³„ìˆ˜ í…Œì´ë¸” ---
    df_coef = pd.DataFrame({
        'Variable': results.params.index,
        'Coef.': results.params.values,
        'Std.Err.': results.std_errors.values,
        'T-Stat': results.tstats.values,
        'P-Value': results.pvalues.values,
        'CI Lower': results.conf_int().iloc[:, 0].values,
        'CI Upper': results.conf_int().iloc[:, 1].values,
    }).round(4)

    # --- êµ¬ë¶„ì„  ---
    separator = pd.DataFrame([["---", "---"]], columns=df_summary.columns)

    # --- í•©ì¹˜ê¸° ---
    df_combined = pd.concat([df_summary, separator, df_coef], ignore_index=True)

    # ì €ì¥
    df_combined.to_csv(filename, index=False, encoding='utf-8-sig')

# =======================================
# 1. ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸° ë° ë³‘í•©
#   - Part1. Sale, Store, ê·¸ ì™¸ ë°ì´í„° ë³‘í•© 
#   - Part2. ì°©í•œê°€ê²©ì—…ì†Œ ë°ì´í„° ë³‘í•© 
#   - Part3. ì„ëŒ€ë£Œ ë°ì´í„° ë³‘í•©
#   - Part4. ìƒê¶Œ-í–‰ì •ë™ shp ë§¤í•‘í…Œì´ë¸” ìƒì„± 
# =======================================
# ---------------------------------------
# Part1. Sale, Store, ê·¸ ì™¸ ë°ì´í„° ë³‘í•© 
# ---------------------------------------

# Sales_ë°ì´í„° 
Sales_2021 = pd.read_csv('./data/ë§¤ì¶œê¸ˆì•¡_2021_í–‰ì •ë™.csv', encoding='utf-8')
Sales_2022 = pd.read_csv('./data/ë§¤ì¶œê¸ˆì•¡_2022_í–‰ì •ë™.csv', encoding='utf-8')
Sales_2023 = pd.read_csv('./data/ë§¤ì¶œê¸ˆì•¡_2023_í–‰ì •ë™.csv', encoding='utf-8')
Sales_2024 = pd.read_csv('./data/ë§¤ì¶œê¸ˆì•¡_2024_í–‰ì •ë™.csv', encoding='cp949')

# ì í¬_ë°ì´í„° 
Stores_2021 = pd.read_csv('./data/ì í¬_2021_í–‰ì •ë™.csv', encoding='utf-8')
Stores_2022 = pd.read_csv('./data/ì í¬_2022_í–‰ì •ë™.csv', encoding='utf-8')
Stores_2023 = pd.read_csv('./data/ì í¬_2023_í–‰ì •ë™.csv', encoding='utf-8')
Stores_2024 = pd.read_csv('./data/ì í¬_2024_í–‰ì •ë™.csv', encoding='cp949')

# ê¸°íƒ€_í†µì œë³€ìˆ˜_ë°ì´í„° 
Indicators = pd.read_csv('./data/ìƒê¶Œë³€í™”ì§€í‘œ_í–‰ì •ë™.csv', encoding='cp949')
Incomes = pd.read_csv('./data/ì†Œë“ê¸ˆì•¡_í–‰ì •ë™.csv', encoding='cp949')
Apartments = pd.read_csv('./data/ì•„íŒŒíŠ¸ë‹¨ì§€ìˆ˜_í–‰ì •ë™.csv', encoding='cp949')
Floatings = pd.read_csv('./data/ìœ ë™ì¸êµ¬ìˆ˜_í–‰ì •ë™.csv', encoding='cp949')
Workers = pd.read_csv('./data/ì§ì¥ì¸êµ¬_í–‰ì •ë™.csv', encoding='cp949')
Facilities = pd.read_csv('./data/ì§‘ê°ì‹œì„¤ìˆ˜_í–‰ì •ë™.csv', encoding='cp949')
Residents = pd.read_csv('./data/ìƒì£¼ì¸êµ¬ìˆ˜_í–‰ì •ë™.csv', encoding='cp949')

# ìš´ì˜_ì˜ì—…_ê°œì›” ì°¨ì´ 
Indicators['ìš´ì˜_ì˜ì—…_ê°œì›”_ì°¨ì´'] = Indicators['ìš´ì˜_ì˜ì—…_ê°œì›”_í‰ê· '] - Indicators['ì„œìš¸_ìš´ì˜_ì˜ì—…_ê°œì›”_í‰ê· ']
Indicators['íì—…_ì˜ì—…_ê°œì›”_ì°¨ì´'] = Indicators['íì—…_ì˜ì—…_ê°œì›”_í‰ê· '] - Indicators['ì„œìš¸_íì—…_ì˜ì—…_ê°œì›”_í‰ê· ']


# ë°ì´í„° 2023~'24ë…„ìœ¼ë¡œ í•œì •í•´ì„œ ë¶„ì„
# ë§¤ì¶œ ë°ì´í„° ë³‘í•©
Sales = pd.concat([Sales_2021, Sales_2022, Sales_2023, Sales_2024], ignore_index=True)

# ì í¬ ë°ì´í„° ë³‘í•©
Stores = pd.concat([Stores_2021, Stores_2022, Stores_2023, Stores_2024], ignore_index=True)


# ê¸°ì¤€_ë…„ë¶„ê¸°_ì½”ë“œ í•„í„° í•¨ìˆ˜ ì •ì˜
def filter_by_year(df):
    return df[df['ê¸°ì¤€_ë…„ë¶„ê¸°_ì½”ë“œ'].astype(str).str[:4].astype(int).between(2021, 2024)]

# í•„í„°ë§ ì ìš©
Sales = filter_by_year(Sales)
Stores = filter_by_year(Stores)
Indicators = filter_by_year(Indicators)
Incomes = filter_by_year(Incomes)
Apartments = filter_by_year(Apartments)
Floatings = filter_by_year(Floatings)
Workers = filter_by_year(Workers)
Facilities = filter_by_year(Facilities)
Residents = filter_by_year(Residents)

# í•„ìš”í•œì»¬ëŸ¼ë§Œ í•„í„° 
Sales= Sales[Sales['ì„œë¹„ìŠ¤_ì—…ì¢…_ì½”ë“œ'].isin(['CS100001','CS100002', 'CS100003', 'CS100004', 'CS100005', 'CS100008'])] 
Stores= Stores[Stores['ì„œë¹„ìŠ¤_ì—…ì¢…_ì½”ë“œ'].isin(['CS100001', 'CS100002', 'CS100003', 'CS100004', 'CS100005', 'CS100008'])] 

Sales_grouped = Sales.groupby(['ê¸°ì¤€_ë…„ë¶„ê¸°_ì½”ë“œ', 'í–‰ì •ë™_ì½”ë“œ','í–‰ì •ë™_ì½”ë“œ_ëª…'])['ë‹¹ì›”_ë§¤ì¶œ_ê¸ˆì•¡'].sum().reset_index()
Stores_grouped = Stores.groupby(['ê¸°ì¤€_ë…„ë¶„ê¸°_ì½”ë“œ', 'í–‰ì •ë™_ì½”ë“œ','í–‰ì •ë™_ì½”ë“œ_ëª…'])[['ìœ ì‚¬_ì—…ì¢…_ì í¬_ìˆ˜','ê°œì—…_ì í¬_ìˆ˜','íì—…_ì í¬_ìˆ˜']].sum().reset_index()

Stores_grouped.rename(columns={'ìœ ì‚¬_ì—…ì¢…_ì í¬_ìˆ˜' : 'ì í¬_ìˆ˜'},inplace=True)
Stores_grouped['ê°œì—…_ë¥ '] = round(Stores_grouped['ê°œì—…_ì í¬_ìˆ˜'] / Stores_grouped['ì í¬_ìˆ˜'],2)
Stores_grouped['íì—…_ë¥ '] = round(Stores_grouped['íì—…_ì í¬_ìˆ˜'] / Stores_grouped['ì í¬_ìˆ˜'],2)

Indicators = Indicators[['ê¸°ì¤€_ë…„ë¶„ê¸°_ì½”ë“œ','í–‰ì •ë™_ì½”ë“œ','ìƒê¶Œ_ë³€í™”_ì§€í‘œ','ìƒê¶Œ_ë³€í™”_ì§€í‘œ_ëª…','ìš´ì˜_ì˜ì—…_ê°œì›”_í‰ê· ','íì—…_ì˜ì—…_ê°œì›”_í‰ê· ','ìš´ì˜_ì˜ì—…_ê°œì›”_ì°¨ì´','íì—…_ì˜ì—…_ê°œì›”_ì°¨ì´']]
Incomes = Incomes[['ê¸°ì¤€_ë…„ë¶„ê¸°_ì½”ë“œ','í–‰ì •ë™_ì½”ë“œ','ì›”_í‰ê· _ì†Œë“_ê¸ˆì•¡','ìŒì‹_ì§€ì¶œ_ì´ê¸ˆì•¡','ì˜ë£Œë¹„_ì§€ì¶œ_ì´ê¸ˆì•¡','êµìœ¡_ì§€ì¶œ_ì´ê¸ˆì•¡']]
Apartments = Apartments[['ê¸°ì¤€_ë…„ë¶„ê¸°_ì½”ë“œ','í–‰ì •ë™_ì½”ë“œ','ì•„íŒŒíŠ¸_ë‹¨ì§€_ìˆ˜','ì•„íŒŒíŠ¸_í‰ê· _ì‹œê°€']]
Floatings = Floatings[['ê¸°ì¤€_ë…„ë¶„ê¸°_ì½”ë“œ','í–‰ì •ë™_ì½”ë“œ','ë‚¨ì„±_ìœ ë™ì¸êµ¬_ìˆ˜','ì—¬ì„±_ìœ ë™ì¸êµ¬_ìˆ˜','ì—°ë ¹ëŒ€_10_ìœ ë™ì¸êµ¬_ìˆ˜','ì—°ë ¹ëŒ€_20_ìœ ë™ì¸êµ¬_ìˆ˜','ì—°ë ¹ëŒ€_30_ìœ ë™ì¸êµ¬_ìˆ˜','ì—°ë ¹ëŒ€_40_ìœ ë™ì¸êµ¬_ìˆ˜','ì—°ë ¹ëŒ€_50_ìœ ë™ì¸êµ¬_ìˆ˜','ì—°ë ¹ëŒ€_60_ì´ìƒ_ìœ ë™ì¸êµ¬_ìˆ˜']]
Workers = Workers[['ê¸°ì¤€_ë…„ë¶„ê¸°_ì½”ë“œ','í–‰ì •ë™_ì½”ë“œ','ë‚¨ì„±_ì§ì¥_ì¸êµ¬_ìˆ˜','ì—¬ì„±_ì§ì¥_ì¸êµ¬_ìˆ˜','ì—°ë ¹ëŒ€_10_ì§ì¥_ì¸êµ¬_ìˆ˜','ì—°ë ¹ëŒ€_20_ì§ì¥_ì¸êµ¬_ìˆ˜','ì—°ë ¹ëŒ€_30_ì§ì¥_ì¸êµ¬_ìˆ˜','ì—°ë ¹ëŒ€_40_ì§ì¥_ì¸êµ¬_ìˆ˜','ì—°ë ¹ëŒ€_50_ì§ì¥_ì¸êµ¬_ìˆ˜','ì—°ë ¹ëŒ€_60_ì´ìƒ_ì§ì¥_ì¸êµ¬_ìˆ˜']]
Facilities = Facilities[['ê¸°ì¤€_ë…„ë¶„ê¸°_ì½”ë“œ','í–‰ì •ë™_ì½”ë“œ','ì§‘ê°ì‹œì„¤_ìˆ˜']]
Residents = Residents[['ê¸°ì¤€_ë…„ë¶„ê¸°_ì½”ë“œ','í–‰ì •ë™_ì½”ë“œ','ì´_ìƒì£¼ì¸êµ¬_ìˆ˜']]

# í†µì‹ ì •ë³´ ì •ì œ 
# ì—°ë ¹ëŒ€ ê·¸ë£¹í•‘ í•¨ìˆ˜
def categorize_age(age):
    if age in [20, 25, 30]:
        return 'age_20_30'
    elif age in [35, 40, 45, 50]:
        return 'age_35_50'
    elif age in [55, 60, 65, 70, 75]:
        return 'age_55_75'
    else:
        return 'other'

# ì²˜ë¦¬í•  ë¶„ê¸° ëª©ë¡
quarters = ['20231','20232','20233','20234','20241','20242','20243','20244']
final_list = []

for quarter in quarters:
    filename = f'í†µì‹ ì •ë³´_{quarter}.csv'

    # CSV íŒŒì¼ ì½ê¸°
    df = pd.read_csv('./data/' + filename)

    # í•„ìš”í•œ ì—´ í•„í„°ë§
    df_filtered = df[['í–‰ì •ë™ì½”ë“œ', 'í–‰ì •ë™', 'ì—°ë ¹ëŒ€', 'ì´ì¸êµ¬ìˆ˜', '1ì¸ê°€êµ¬ìˆ˜']].copy()

    # ìˆ«ìí˜•ìœ¼ë¡œ ë³€í™˜
    df_filtered['ì´ì¸êµ¬ìˆ˜'] = df_filtered['ì´ì¸êµ¬ìˆ˜'].str.replace(',', '', regex=False).astype(float)
    df_filtered['1ì¸ê°€êµ¬ìˆ˜'] = df_filtered['1ì¸ê°€êµ¬ìˆ˜'].str.replace(',', '', regex=False).astype(float)

    # ì—°ë ¹ëŒ€ ê·¸ë£¹ ë¶„ë¥˜
    df_filtered['age_group'] = df_filtered['ì—°ë ¹ëŒ€'].apply(categorize_age)

    # ì´ ì¸êµ¬ìˆ˜ ë° 1ì¸ ê°€êµ¬ìˆ˜ ì§‘ê³„
    total_pop = df_filtered.groupby(['í–‰ì •ë™ì½”ë“œ', 'í–‰ì •ë™'])['ì´ì¸êµ¬ìˆ˜'].sum().reset_index(name='ì´ì¸êµ¬ìˆ˜_í•©')
    single_households = df_filtered.groupby(['í–‰ì •ë™ì½”ë“œ', 'í–‰ì •ë™'])['1ì¸ê°€êµ¬ìˆ˜'].sum().reset_index(name='1ì¸ê°€êµ¬ìˆ˜_í•©')

    # ì—°ë ¹ëŒ€ë³„ ì¸êµ¬ ì§‘ê³„
    age_group_pop = df_filtered[df_filtered['age_group'] != 'other']
    age_group_sum = age_group_pop.groupby(['í–‰ì •ë™ì½”ë“œ', 'í–‰ì •ë™', 'age_group'])['ì´ì¸êµ¬ìˆ˜'].sum().unstack(fill_value=0).reset_index()

    # ë³‘í•©
    df_merged = total_pop.merge(single_households, on=['í–‰ì •ë™ì½”ë“œ', 'í–‰ì •ë™']).merge(age_group_sum, on=['í–‰ì •ë™ì½”ë“œ', 'í–‰ì •ë™'])

    # ìˆ«ìí˜• ë³€í™˜ í›„ ë¹„ìœ¨ ê³„ì‚°
    df_merged['ì´ì¸êµ¬ìˆ˜_í•©'] = pd.to_numeric(df_merged['ì´ì¸êµ¬ìˆ˜_í•©'], errors='coerce')
    df_merged['1ì¸ê°€êµ¬ìˆ˜_í•©'] = pd.to_numeric(df_merged['1ì¸ê°€êµ¬ìˆ˜_í•©'], errors='coerce')

    df_merged['1ì¸_ê°€êµ¬ë¹„'] = df_merged['1ì¸ê°€êµ¬ìˆ˜_í•©'] / df_merged['ì´ì¸êµ¬ìˆ˜_í•©']
    df_merged['20_30_ì¸êµ¬ë¹„'] = df_merged.get('age_20_30', 0) / df_merged['ì´ì¸êµ¬ìˆ˜_í•©']
    df_merged['31_50_ì¸êµ¬ë¹„'] = df_merged.get('age_35_50', 0) / df_merged['ì´ì¸êµ¬ìˆ˜_í•©']
    df_merged['51_75_ì¸êµ¬ë¹„'] = df_merged.get('age_55_75', 0) / df_merged['ì´ì¸êµ¬ìˆ˜_í•©']

    # ê¸°ì¤€ ì½”ë“œ ì¶”ê°€
    df_merged['ê¸°ì¤€_ë…„ë¶„ê¸°_ì½”ë“œ'] = quarter

    # ê²°ê³¼ ì €ì¥
    result_df = df_merged[['ê¸°ì¤€_ë…„ë¶„ê¸°_ì½”ë“œ', 'í–‰ì •ë™ì½”ë“œ', 'í–‰ì •ë™', 'ì´ì¸êµ¬ìˆ˜_í•©','1ì¸ê°€êµ¬ìˆ˜_í•©', '1ì¸_ê°€êµ¬ë¹„', '20_30_ì¸êµ¬ë¹„', '31_50_ì¸êµ¬ë¹„', '51_75_ì¸êµ¬ë¹„']]
    final_list.append(result_df)

# ëª¨ë“  ë¶„ê¸° ë³‘í•©
Population = pd.concat(final_list, ignore_index=True)
# í˜•ì‹ í†µì¼ 
Population['ê¸°ì¤€_ë…„ë¶„ê¸°_ì½”ë“œ'] = Population['ê¸°ì¤€_ë…„ë¶„ê¸°_ì½”ë“œ'].astype(int)
# ë³‘í•© í‚¤ ì„¤ì •
merge_keys = ['ê¸°ì¤€_ë…„ë¶„ê¸°_ì½”ë“œ','í–‰ì •ë™_ì½”ë“œ']

# Sales ê¸°ì¤€ìœ¼ë¡œ ì»¬ëŸ¼ ë‹¨ìœ„ ë³‘í•©
df_ìƒê¶Œë°ì´í„° = Sales_grouped.merge(Stores_grouped, on=merge_keys, how='left') \
                          .merge(Indicators, on=merge_keys, how='left') \
                          .merge(Incomes, on=merge_keys, how='left') \
                          .merge(Apartments, on=merge_keys, how='left') \
                          .merge(Floatings, on=merge_keys, how='left') \
                          .merge(Workers, on=merge_keys, how='left') \
                          .merge(Facilities, on=merge_keys, how='left') \
                          .merge(Residents, on=merge_keys, how='left') 

# Population ì½”ë“œëª… ê¸°ì¤€ ì¡°ì¸
df_ìƒê¶Œë°ì´í„°.rename(columns={'í–‰ì •ë™_ì½”ë“œ_ëª…_x':'í–‰ì •ë™'},inplace=True)
df_ìƒê¶Œë°ì´í„°.loc[df_ìƒê¶Œë°ì´í„°['í–‰ì •ë™_ì½”ë“œ'] == 11620685, 'í–‰ì •ë™'] = 'ì‹ ì‚¬ë™(ê´€ì•…)'
df_ìƒê¶Œë°ì´í„°.loc[df_ìƒê¶Œë°ì´í„°['í–‰ì •ë™_ì½”ë“œ'] == 11680510, 'í–‰ì •ë™'] = 'ì‹ ì‚¬ë™(ê°•ë‚¨)'
df_ìƒê¶Œë°ì´í„°['í–‰ì •ë™'] = df_ìƒê¶Œë°ì´í„°['í–‰ì •ë™'].str.replace('?', 'Â·', regex=False)

Population.loc[(Population['í–‰ì •ë™ì½”ë“œ'] == 1121068) & (Population['í–‰ì •ë™'] == 'ì‹ ì‚¬ë™'), 'í–‰ì •ë™'] = 'ì‹ ì‚¬ë™(ê´€ì•…)'
Population.loc[(Population['í–‰ì •ë™ì½”ë“œ'] == 1123051) & (Population['í–‰ì •ë™'] == 'ì‹ ì‚¬ë™'), 'í–‰ì •ë™'] = 'ì‹ ì‚¬ë™(ê°•ë‚¨)'

df_ìƒê¶Œë°ì´í„° = df_ìƒê¶Œë°ì´í„°.merge(Population, on=['ê¸°ì¤€_ë…„ë¶„ê¸°_ì½”ë“œ','í–‰ì •ë™'], how='left')

# ê²°ì¸¡ì¹˜ ëŒ€ì²´í•  ì»¬ëŸ¼ ë¦¬ìŠ¤íŠ¸
job_cols = [
    'ë‚¨ì„±_ì§ì¥_ì¸êµ¬_ìˆ˜', 'ì—¬ì„±_ì§ì¥_ì¸êµ¬_ìˆ˜',
    'ì—°ë ¹ëŒ€_10_ì§ì¥_ì¸êµ¬_ìˆ˜', 'ì—°ë ¹ëŒ€_20_ì§ì¥_ì¸êµ¬_ìˆ˜', 'ì—°ë ¹ëŒ€_30_ì§ì¥_ì¸êµ¬_ìˆ˜',
    'ì—°ë ¹ëŒ€_40_ì§ì¥_ì¸êµ¬_ìˆ˜', 'ì—°ë ¹ëŒ€_50_ì§ì¥_ì¸êµ¬_ìˆ˜', 'ì—°ë ¹ëŒ€_60_ì´ìƒ_ì§ì¥_ì¸êµ¬_ìˆ˜',
    'ì´ì¸êµ¬ìˆ˜_í•©','1ì¸ê°€êµ¬ìˆ˜_í•©','1ì¸_ê°€êµ¬ë¹„','20_30_ì¸êµ¬ë¹„','31_50_ì¸êµ¬ë¹„','51_75_ì¸êµ¬ë¹„'
]

# ë¶„ê¸° ê¸°ì¤€ìœ¼ë¡œ ê·¸ë£¹ë³„ í‰ê·  êµ¬í•˜ê¸°
grouped_means = df_ìƒê¶Œë°ì´í„°.groupby('ê¸°ì¤€_ë…„ë¶„ê¸°_ì½”ë“œ')[job_cols].transform('mean')

# ê²°ì¸¡ì¹˜ë¥¼ ë¶„ê¸°ë³„ í‰ê· ê°’ìœ¼ë¡œ ëŒ€ì²´
df_ìƒê¶Œë°ì´í„°[job_cols] = df_ìƒê¶Œë°ì´í„°[job_cols].fillna(grouped_means)
df_ìƒê¶Œë°ì´í„°.info()

# export
df_ìƒê¶Œë°ì´í„°.to_csv('ìƒê¶Œë°ì´í„°.csv',encoding='utf-8-sig', index=False)

# ---------------------------------------
# Part2. ì°©í•œê°€ê²©ì—…ì†Œ ë°ì´í„° ë³‘í•© 
# ---------------------------------------

# --- 1. ì¢Œí‘œ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜
def get_coords_from_address(address):
    url = 'https://dapi.kakao.com/v2/local/search/address.json' # ì¹´ì¹´ì˜¤ ì£¼ì†Œ í˜¸ì¶œ API
    headers = {'Authorization': f'KakaoAK {'386797ea7e88e3189c4ae3389f5e13c6'}'}
    params = {"query": address}

    try:
        res = requests.get(url, headers=headers, params=params, timeout=5)
        res.raise_for_status()  # HTTP ì—ëŸ¬ ë°œìƒ ì‹œ ì˜ˆì™¸ ë°œìƒ
        data = res.json()
        
        if data.get('documents'):
            doc = data['documents'][0]
            return float(doc['x']), float(doc['y'])  # (ê²½ë„, ìœ„ë„)
    
    except requests.exceptions.HTTPError as e:
        print(f"[HTTPError] ì£¼ì†Œ ìš”ì²­ ì‹¤íŒ¨ - {address} | {e}")
    except requests.exceptions.ConnectionError as e:
        print(f"[ConnectionError] ì¸í„°ë„· ì—°ê²° ì˜¤ë¥˜ - {address} | {e}")
    except requests.exceptions.Timeout as e:
        print(f"[Timeout] ìš”ì²­ ì‹œê°„ ì´ˆê³¼ - {address} | {e}")
    except requests.exceptions.RequestException as e:
        print(f"[RequestException] ê¸°íƒ€ ìš”ì²­ ì˜¤ë¥˜ - {address} | {e}")
    except Exception as e:
        print(f"[UnknownError] ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜ - {address} | {e}")
    
    return None, None

# --- 2. ì¢Œí‘œ â†’ í–‰ì •ë™ ì½”ë“œ/ëª…
def get_region_code_from_coords(x, y):
    url = "https://api.vworld.kr/req/address" # êµ­í† ë¶€ ë””ì§€í„¸ íŠ¸ìœˆêµ­í†  ì£¼ì†Œ API
    params = {
        "service": "address",
        "request": "getAddress",
        "point": f"{x},{y}",  # ê²½ë„, ìœ„ë„ ìˆœì„œ
        "crs": "EPSG:4326",
        "format": "json",
        "type": "both",
        "key": '248F6D1B-0D46-3D34-85E2-0463D838D5CB'
    }

    try:
        response = requests.get(url, params=params, timeout=5)
        response.raise_for_status()  # HTTP ìƒíƒœì½”ë“œê°€ 4xx/5xxë©´ ì˜ˆì™¸ ë°œìƒ
        data = response.json()

        if data['response']['status'] == 'OK':
            # "type"ì´ "road"ì¸ ê²°ê³¼ë§Œ í•„í„°ë§
            result = data['response']['result'][1]

            # í–‰ì •ë™ ì¶”ì¶œ: í–‰ì •ë™ ì—†ìœ¼ë©´ ë²•ì •ë™ fallback
            dong_name = result.get('level4A') or result.get('level4L')
            dong_code = result.get('level4AC') or result.get('level4LC')

            return dong_name, dong_code
        else:
            print(f"[API Response Error] ìƒíƒœ: {data['response']['status']} | ì¢Œí‘œ: ({x}, {y})")
            return None, None

    except requests.exceptions.HTTPError as e:
        print(f"[HTTPError] ì‘ë‹µ ì½”ë“œ ì˜¤ë¥˜ | ì¢Œí‘œ: ({x}, {y}) | {e}")
    except requests.exceptions.ConnectionError as e:
        print(f"[ConnectionError] ì—°ê²° ì‹¤íŒ¨ | ì¢Œí‘œ: ({x}, {y}) | {e}")
    except requests.exceptions.Timeout as e:
        print(f"[Timeout] ì‘ë‹µ ì§€ì—° | ì¢Œí‘œ: ({x}, {y}) | {e}")
    except requests.exceptions.RequestException as e:
        print(f"[RequestException] ìš”ì²­ ì‹¤íŒ¨ | ì¢Œí‘œ: ({x}, {y}) | {e}")
    except (KeyError, IndexError) as e:
        print(f"[ParsingError] ê²°ê³¼ êµ¬ì¡° íŒŒì‹± ì‹¤íŒ¨ | ì¢Œí‘œ: ({x}, {y}) | {e}")
    except Exception as e:
        print(f"[UnknownError] ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜ | ì¢Œí‘œ: ({x}, {y}) | {e}")

    return None, None

# --- 3. ì£¼ì†Œ ë¦¬ìŠ¤íŠ¸ë¥¼ ë°›ì•„ tqdm ì ìš©í•˜ë©° í–‰ì •ë™ ì •ë³´ ë°˜í™˜
def get_dong_info_parallel(addresses, max_workers=10):
    results = []

    def worker(address):
        x, y = get_coords_from_address(address)
        if x is not None and y is not None:
            dong_name, dong_code = get_region_code_from_coords(x, y)
        else:
            dong_name, dong_code = None, None
        return {'ì£¼ì†Œ': address, 'í–‰ì •ë™_ëª…': dong_name, 'í–‰ì •ë™_ì½”ë“œ': dong_code}

    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        futures = {executor.submit(worker, addr): addr for addr in addresses}
        for future in tqdm(as_completed(futures), total=len(futures), desc="ë³‘ë ¬ í–‰ì •ë™ ë§¤í•‘ ì¤‘"):
            result = future.result()
            results.append(result)

    return pd.DataFrame(results)

# ì°©í•œê°€ê²©ì—…ì†Œ_2023~2024
GoodPrices_Data = {
    "20233": "./data/ì°©í•œê°€ê²©ì—…ì†Œ_20233.csv",
    "20241": "./data/ì°©í•œê°€ê²©ì—…ì†Œ_20241.csv",
    "20242": "./data/ì°©í•œê°€ê²©ì—…ì†Œ_20242.csv",
    "20243": "./data/ì°©í•œê°€ê²©ì—…ì†Œ_20243.csv",
    "20244": "./data/ì°©í•œê°€ê²©ì—…ì†Œ_20244.csv"
}

df_list = []
for quarter, path in GoodPrices_Data.items():
    df = pd.read_csv(path, encoding='cp949')  # í•„ìš”ì‹œ encoding='cp949'
    df['ê¸°ì¤€_ë…„ë¶„ê¸°_ì½”ë“œ'] = quarter           # ë¶„ê¸° ì»¬ëŸ¼ ì¶”ê°€
    df_list.append(df)


# 1. í•˜ë‚˜ì˜ ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ ë³‘í•©
GoodPrices = pd.concat(df_list, ignore_index=True)
GoodPrices_ì„œìš¸íŠ¹ë³„ì‹œ = GoodPrices[GoodPrices['ì‹œë„'] =='ì„œìš¸íŠ¹ë³„ì‹œ']

# 2. ì£¼ì†Œ í–‰ì •ë™ ë§¤í•‘ 
df_ì£¼ì†Œ_í–‰ì •ë™ë§¤í•‘ = get_dong_info_parallel(GoodPrices_ì„œìš¸íŠ¹ë³„ì‹œ['ì£¼ì†Œ'])
df_ì£¼ì†Œ_í–‰ì •ë™ë§¤í•‘ = df_ì£¼ì†Œ_í–‰ì •ë™ë§¤í•‘.drop_duplicates(subset='ì£¼ì†Œ', keep='first')

# 3. ë³‘í•© 
GoodPrices_ì„œìš¸íŠ¹ë³„ì‹œ = GoodPrices_ì„œìš¸íŠ¹ë³„ì‹œ.merge(df_ì£¼ì†Œ_í–‰ì •ë™ë§¤í•‘,on=['ì£¼ì†Œ'],how='left')
GoodPrices_ì„œìš¸íŠ¹ë³„ì‹œ.to_csv('ì°©í•œê°€ê²©ì—…ì†Œ.csv',encoding='utf-8-sig', index=False)

# 4. í–‰ì •ë™ ë§¤í•‘ ì•ˆëœ ê³³ ì¶”ê°€ì •ì œ (ìˆ˜ê¸°)
GoodPrices_ì„œìš¸íŠ¹ë³„ì‹œ[GoodPrices_ì„œìš¸íŠ¹ë³„ì‹œ['í–‰ì •ë™_ëª…'].isna()]
# -----------------------------------------------
# Part3. ì„ëŒ€ë£Œ ë°ì´í„° ë³‘í•© 
# -----------------------------------------------
ì¤‘ëŒ€í˜•ìƒê°€_20211 = pd.read_csv('./data/ì¤‘ëŒ€í˜•ìƒê°€_20211.csv', encoding='utf-8')
ì¤‘ëŒ€í˜•ìƒê°€_20212 = pd.read_csv('./data/ì¤‘ëŒ€í˜•ìƒê°€_20212.csv', encoding='utf-8')
ì¤‘ëŒ€í˜•ìƒê°€_20213 = pd.read_csv('./data/ì¤‘ëŒ€í˜•ìƒê°€_20213.csv', encoding='utf-8')
ì¤‘ëŒ€í˜•ìƒê°€_20214 = pd.read_csv('./data/ì¤‘ëŒ€í˜•ìƒê°€_20214.csv', encoding='utf-8')

ì¤‘ëŒ€í˜•ìƒê°€_20221 = pd.read_csv('./data/ì¤‘ëŒ€í˜•ìƒê°€_20221.csv', encoding='utf-8')
ì¤‘ëŒ€í˜•ìƒê°€_20222 = pd.read_csv('./data/ì¤‘ëŒ€í˜•ìƒê°€_20222.csv', encoding='utf-8')
ì¤‘ëŒ€í˜•ìƒê°€_20223 = pd.read_csv('./data/ì¤‘ëŒ€í˜•ìƒê°€_20223.csv', encoding='utf-8')
ì¤‘ëŒ€í˜•ìƒê°€_20224 = pd.read_csv('./data/ì¤‘ëŒ€í˜•ìƒê°€_20224.csv', encoding='utf-8')

ì¤‘ëŒ€í˜•ìƒê°€_20231 = pd.read_csv('./data/ì¤‘ëŒ€í˜•ìƒê°€_20231.csv', encoding='utf-8')
ì¤‘ëŒ€í˜•ìƒê°€_20232 = pd.read_csv('./data/ì¤‘ëŒ€í˜•ìƒê°€_20232.csv', encoding='utf-8')
ì¤‘ëŒ€í˜•ìƒê°€_20233 = pd.read_csv('./data/ì¤‘ëŒ€í˜•ìƒê°€_20233.csv', encoding='utf-8')
ì¤‘ëŒ€í˜•ìƒê°€_20234 = pd.read_csv('./data/ì¤‘ëŒ€í˜•ìƒê°€_20234.csv', encoding='utf-8')

ì¤‘ëŒ€í˜•ìƒê°€_20241 = pd.read_csv('./data/ì¤‘ëŒ€í˜•ìƒê°€_20241.csv', encoding='cp949')
ì¤‘ëŒ€í˜•ìƒê°€_20242 = pd.read_csv('./data/ì¤‘ëŒ€í˜•ìƒê°€_20242.csv', encoding='cp949')
ì¤‘ëŒ€í˜•ìƒê°€_20243 = pd.read_csv('./data/ì¤‘ëŒ€í˜•ìƒê°€_20243.csv', encoding='cp949')
ì¤‘ëŒ€í˜•ìƒê°€_20244 = pd.read_csv('./data/ì¤‘ëŒ€í˜•ìƒê°€_20244.csv', encoding='cp949')
ì¤‘ëŒ€í˜•ìƒê°€_20251 = pd.read_csv('./data/ì¤‘ëŒ€í˜•ìƒê°€_20251.csv', encoding='cp949')


# ì„œìš¸íŠ¹ë³„ì‹œ & ë¹„ìƒê¶Œ ì œì™¸ í•„í„°ë§ í•¨ìˆ˜
def í•„í„°ë§(df):
    return df[
        df['ì†Œì¬ì§€'].str.startswith("ì„œìš¸íŠ¹ë³„ì‹œ") &
        (df['ìƒê¶Œëª…'] != '0.ë¹„ìƒê¶Œ')
    ]

# í•„í„°ë§ ì ìš©
ì¤‘ëŒ€í˜•ìƒê°€_20211 = í•„í„°ë§(ì¤‘ëŒ€í˜•ìƒê°€_20211)
ì¤‘ëŒ€í˜•ìƒê°€_20212 = í•„í„°ë§(ì¤‘ëŒ€í˜•ìƒê°€_20212)
ì¤‘ëŒ€í˜•ìƒê°€_20213 = í•„í„°ë§(ì¤‘ëŒ€í˜•ìƒê°€_20213)
ì¤‘ëŒ€í˜•ìƒê°€_20214 = í•„í„°ë§(ì¤‘ëŒ€í˜•ìƒê°€_20214)

ì¤‘ëŒ€í˜•ìƒê°€_20221 = í•„í„°ë§(ì¤‘ëŒ€í˜•ìƒê°€_20221)
ì¤‘ëŒ€í˜•ìƒê°€_20222 = í•„í„°ë§(ì¤‘ëŒ€í˜•ìƒê°€_20222)
ì¤‘ëŒ€í˜•ìƒê°€_20223 = í•„í„°ë§(ì¤‘ëŒ€í˜•ìƒê°€_20223)
ì¤‘ëŒ€í˜•ìƒê°€_20224 = í•„í„°ë§(ì¤‘ëŒ€í˜•ìƒê°€_20224)

ì¤‘ëŒ€í˜•ìƒê°€_20231 = í•„í„°ë§(ì¤‘ëŒ€í˜•ìƒê°€_20231)
ì¤‘ëŒ€í˜•ìƒê°€_20232 = í•„í„°ë§(ì¤‘ëŒ€í˜•ìƒê°€_20232)
ì¤‘ëŒ€í˜•ìƒê°€_20233 = í•„í„°ë§(ì¤‘ëŒ€í˜•ìƒê°€_20233)
ì¤‘ëŒ€í˜•ìƒê°€_20234 = í•„í„°ë§(ì¤‘ëŒ€í˜•ìƒê°€_20234)

ì¤‘ëŒ€í˜•ìƒê°€_20241 = í•„í„°ë§(ì¤‘ëŒ€í˜•ìƒê°€_20241)
ì¤‘ëŒ€í˜•ìƒê°€_20242 = í•„í„°ë§(ì¤‘ëŒ€í˜•ìƒê°€_20242)
ì¤‘ëŒ€í˜•ìƒê°€_20243 = í•„í„°ë§(ì¤‘ëŒ€í˜•ìƒê°€_20243)
ì¤‘ëŒ€í˜•ìƒê°€_20244 = í•„í„°ë§(ì¤‘ëŒ€í˜•ìƒê°€_20244)
ì¤‘ëŒ€í˜•ìƒê°€_20251 = í•„í„°ë§(ì¤‘ëŒ€í˜•ìƒê°€_20251)

# ë¶„ê¸°ë³„ ì›”ì„¸ ì»¬ëŸ¼ ëª©ë¡
ë¶„ê¸°ì»¬ëŸ¼ = ['ì œ1ì›”ì‹œì¥ì„ëŒ€ë£Œ_ã¡ë‹¹ì›”ì„¸ì„ëŒ€ë£Œ', 'ì œ2ì›”ì‹œì¥ì„ëŒ€ë£Œ_ã¡ë‹¹ì›”ì„¸ì„ëŒ€ë£Œ', 'ì œ3ì›”ì‹œì¥ì„ëŒ€ë£Œ_ã¡ë‹¹ì›”ì„¸ì„ëŒ€ë£Œ']

# ë¶„ê¸°ë³„ í‰ê·  ê³„ì‚° í•¨ìˆ˜
def ë¶„ê¸°í‰ê· (df, ì»¬ëŸ¼ë¦¬ìŠ¤íŠ¸, ë¶„ê¸°ì´ë¦„):
    grouped = df.groupby('ìƒê¶Œëª…')[ì»¬ëŸ¼ë¦¬ìŠ¤íŠ¸].median()
    grouped['í‰ê· ì„ëŒ€ë£Œ'] = grouped.mean(axis=1)
    grouped['ê¸°ì¤€_ë…„ë¶„ê¸°_ì½”ë“œ'] = ë¶„ê¸°ì´ë¦„
    return grouped[['ê¸°ì¤€_ë…„ë¶„ê¸°_ì½”ë“œ','í‰ê· ì„ëŒ€ë£Œ']].reset_index()

# ì»¬ëŸ¼ëª… í†µì¼
dfs = [ì¤‘ëŒ€í˜•ìƒê°€_20211, ì¤‘ëŒ€í˜•ìƒê°€_20212, ì¤‘ëŒ€í˜•ìƒê°€_20213, ì¤‘ëŒ€í˜•ìƒê°€_20214, ì¤‘ëŒ€í˜•ìƒê°€_20221, ì¤‘ëŒ€í˜•ìƒê°€_20222, ì¤‘ëŒ€í˜•ìƒê°€_20223, ì¤‘ëŒ€í˜•ìƒê°€_20224]  # í•„ìš”í•œ ë°ì´í„°í”„ë ˆì„ ë¦¬ìŠ¤íŠ¸

for df in dfs:
    df.rename(columns={'ì œ1ì›”ì‹œì¥ì„ëŒ€ë£Œ_më‹¹ì›”ì„¸ì„ëŒ€ë£Œ' : 'ì œ1ì›”ì‹œì¥ì„ëŒ€ë£Œ_ã¡ë‹¹ì›”ì„¸ì„ëŒ€ë£Œ', 
                       'ì œ2ì›”ì‹œì¥ì„ëŒ€ë£Œ_më‹¹ì›”ì„¸ì„ëŒ€ë£Œ' : 'ì œ2ì›”ì‹œì¥ì„ëŒ€ë£Œ_ã¡ë‹¹ì›”ì„¸ì„ëŒ€ë£Œ',
                       'ì œ3ì›”ì‹œì¥ì„ëŒ€ë£Œ_më‹¹ì›”ì„¸ì„ëŒ€ë£Œ' : 'ì œ3ì›”ì‹œì¥ì„ëŒ€ë£Œ_ã¡ë‹¹ì›”ì„¸ì„ëŒ€ë£Œ'}, inplace=True)

# ê°ê°ì˜ ë¶„ê¸°ë³„ í‰ê·  ê³„ì‚°
ì§€ì—­ë³„_2021_1ë¶„ê¸°_í‰ê·  = ë¶„ê¸°í‰ê· (ì¤‘ëŒ€í˜•ìƒê°€_20211, ë¶„ê¸°ì»¬ëŸ¼, '20211')
ì§€ì—­ë³„_2021_2ë¶„ê¸°_í‰ê·  = ë¶„ê¸°í‰ê· (ì¤‘ëŒ€í˜•ìƒê°€_20212, ë¶„ê¸°ì»¬ëŸ¼, '20212')
ì§€ì—­ë³„_2021_3ë¶„ê¸°_í‰ê·  = ë¶„ê¸°í‰ê· (ì¤‘ëŒ€í˜•ìƒê°€_20213, ë¶„ê¸°ì»¬ëŸ¼, '20213')
ì§€ì—­ë³„_2021_4ë¶„ê¸°_í‰ê·  = ë¶„ê¸°í‰ê· (ì¤‘ëŒ€í˜•ìƒê°€_20214, ë¶„ê¸°ì»¬ëŸ¼, '20214')

ì§€ì—­ë³„_2022_1ë¶„ê¸°_í‰ê·  = ë¶„ê¸°í‰ê· (ì¤‘ëŒ€í˜•ìƒê°€_20221, ë¶„ê¸°ì»¬ëŸ¼, '20221')
ì§€ì—­ë³„_2022_2ë¶„ê¸°_í‰ê·  = ë¶„ê¸°í‰ê· (ì¤‘ëŒ€í˜•ìƒê°€_20222, ë¶„ê¸°ì»¬ëŸ¼, '20222')
ì§€ì—­ë³„_2022_3ë¶„ê¸°_í‰ê·  = ë¶„ê¸°í‰ê· (ì¤‘ëŒ€í˜•ìƒê°€_20223, ë¶„ê¸°ì»¬ëŸ¼, '20223')
ì§€ì—­ë³„_2022_4ë¶„ê¸°_í‰ê·  = ë¶„ê¸°í‰ê· (ì¤‘ëŒ€í˜•ìƒê°€_20224, ë¶„ê¸°ì»¬ëŸ¼, '20224')

ì§€ì—­ë³„_2023_1ë¶„ê¸°_í‰ê·  = ë¶„ê¸°í‰ê· (ì¤‘ëŒ€í˜•ìƒê°€_20231, ë¶„ê¸°ì»¬ëŸ¼, '20231')
ì§€ì—­ë³„_2023_2ë¶„ê¸°_í‰ê·  = ë¶„ê¸°í‰ê· (ì¤‘ëŒ€í˜•ìƒê°€_20232, ë¶„ê¸°ì»¬ëŸ¼, '20232')
ì§€ì—­ë³„_2023_3ë¶„ê¸°_í‰ê·  = ë¶„ê¸°í‰ê· (ì¤‘ëŒ€í˜•ìƒê°€_20233, ë¶„ê¸°ì»¬ëŸ¼, '20233')
ì§€ì—­ë³„_2023_4ë¶„ê¸°_í‰ê·  = ë¶„ê¸°í‰ê· (ì¤‘ëŒ€í˜•ìƒê°€_20234, ë¶„ê¸°ì»¬ëŸ¼, '20234')

ì§€ì—­ë³„_2024_1ë¶„ê¸°_í‰ê·  = ë¶„ê¸°í‰ê· (ì¤‘ëŒ€í˜•ìƒê°€_20241, ë¶„ê¸°ì»¬ëŸ¼, '20241')
ì§€ì—­ë³„_2024_2ë¶„ê¸°_í‰ê·  = ë¶„ê¸°í‰ê· (ì¤‘ëŒ€í˜•ìƒê°€_20242, ë¶„ê¸°ì»¬ëŸ¼, '20242')
ì§€ì—­ë³„_2024_3ë¶„ê¸°_í‰ê·  = ë¶„ê¸°í‰ê· (ì¤‘ëŒ€í˜•ìƒê°€_20243, ë¶„ê¸°ì»¬ëŸ¼, '20243')
ì§€ì—­ë³„_2024_4ë¶„ê¸°_í‰ê·  = ë¶„ê¸°í‰ê· (ì¤‘ëŒ€í˜•ìƒê°€_20244, ë¶„ê¸°ì»¬ëŸ¼, '20244')
ì§€ì—­ë³„_2025_1ë¶„ê¸°_í‰ê·  = ë¶„ê¸°í‰ê· (ì¤‘ëŒ€í˜•ìƒê°€_20251, ë¶„ê¸°ì»¬ëŸ¼, '20251')

# ë³‘í•©
ì§€ì—­ë³„_ì„ëŒ€ë£Œ = pd.concat([
    ì§€ì—­ë³„_2021_1ë¶„ê¸°_í‰ê· ,
    ì§€ì—­ë³„_2021_2ë¶„ê¸°_í‰ê· ,
    ì§€ì—­ë³„_2021_3ë¶„ê¸°_í‰ê· ,
    ì§€ì—­ë³„_2021_4ë¶„ê¸°_í‰ê· ,
    ì§€ì—­ë³„_2022_1ë¶„ê¸°_í‰ê· ,
    ì§€ì—­ë³„_2022_2ë¶„ê¸°_í‰ê· ,
    ì§€ì—­ë³„_2022_3ë¶„ê¸°_í‰ê· ,
    ì§€ì—­ë³„_2022_4ë¶„ê¸°_í‰ê· ,
    ì§€ì—­ë³„_2023_1ë¶„ê¸°_í‰ê· ,
    ì§€ì—­ë³„_2023_2ë¶„ê¸°_í‰ê· ,
    ì§€ì—­ë³„_2023_3ë¶„ê¸°_í‰ê· ,
    ì§€ì—­ë³„_2023_4ë¶„ê¸°_í‰ê· ,
    ì§€ì—­ë³„_2024_1ë¶„ê¸°_í‰ê· ,
    ì§€ì—­ë³„_2024_2ë¶„ê¸°_í‰ê· ,
    ì§€ì—­ë³„_2024_3ë¶„ê¸°_í‰ê· ,
    ì§€ì—­ë³„_2024_4ë¶„ê¸°_í‰ê· ,
    ì§€ì—­ë³„_2025_1ë¶„ê¸°_í‰ê· 
], axis=0, ignore_index=True)

# ---------------------------------------------------------------------------------------------------------
# ìƒê¶Œ-í–‰ì •ë™ ê³µê°„ì •ë³´ì¡°ì¸ì„ í†µí•´ ì„ëŒ€ë£Œ ìƒê¶Œì˜ í–‰ì •ë™ì½”ë“œë¥¼ ë§¤í•‘í•˜ì˜€ìŒ
# ---------------------------------------------------------------------------------------------------------

# 1. ìƒê¶Œ êµ¬íšë„ ë¡œë“œ ë° ì¢Œí‘œê³„ ì„¤ì •
gdf_ìƒê¶Œ = gpd.read_file("./data/ìµœì¢…ìƒê¶Œ368.shp").to_crs(epsg=4326)
gdf_ìƒê¶Œ = gdf_ìƒê¶Œ[gdf_ìƒê¶Œ['ì‹œë„ì½”ë“œ'] == '11']

gdf_í–‰ì •ë™ = gpd.read_file("./data/sig.shp", encoding='utf-8')
gdf_í–‰ì •ë™ = gdf_í–‰ì •ë™.set_crs(epsg=5181).to_crs(epsg=4326)

# 2. ìƒê¶Œ ì¤‘ì‹¬ì  GeoDataFrame ìƒì„±
gdf_centroids = gpd.GeoDataFrame(
    gdf_ìƒê¶Œ.drop(columns='geometry'),  # ê¸°ì¡´ geometry ì œê±°
    geometry=gdf_ìƒê¶Œ.geometry.centroid,
    crs=gdf_ìƒê¶Œ.crs
)

# 3. ê³µê°„ì¡°ì¸: ì¤‘ì‹¬ì ì´ í¬í•¨ë˜ëŠ” í–‰ì •ë™ ì°¾ê¸°
gdf_ë§¤í•‘ = gpd.sjoin(
    gdf_centroids,
    gdf_í–‰ì •ë™[['ADSTRD_CD', 'ADSTRD_NM', 'geometry']],
    how='left',
    predicate='within'  # ì¤‘ì‹¬ì ì´ í–‰ì •ë™ ê²½ê³„ ë‚´ì— ìˆëŠ”ì§€
)

# 4. ì»¬ëŸ¼ ì´ë¦„ ì •ë¦¬ (ì„ íƒ)
gdf_ë§¤í•‘ = gdf_ë§¤í•‘.rename(columns={'ADSTRD_CD': 'í–‰ì •ë™_ì½”ë“œ', 'ADSTRD_NM': 'í–‰ì •ë™_ëª…'})
gdf_ë§¤í•‘ = gdf_ë§¤í•‘[['ìƒê¶Œëª…', 'í–‰ì •ë™_ëª…', 'í–‰ì •ë™_ì½”ë“œ']]

# 6. ì§€ì—­ë³„ ì„ëŒ€ë£Œì— ë³‘í•© 
ì§€ì—­ë³„_ì„ëŒ€ë£Œ = ì§€ì—­ë³„_ì„ëŒ€ë£Œ.merge(gdf_ë§¤í•‘, on=['ìƒê¶Œëª…'], how='left')
ì§€ì—­ë³„_ì„ëŒ€ë£Œ.to_csv('ì§€ì—­ë³„_ì„ëŒ€ë£Œ.csv',encoding='utf-8-sig', index=False)

# ===============================================================================
# 2. ë¶„ì„ì„ ìœ„í•œ íŒŒìƒë³€ìˆ˜ ìƒì„± 
#
# [[Part1. ì°©í•œê°€ê²©ì—…ì†Œ]]
#   1. ê¸°ì¤€_ë…„ë¶„ê¸°_ì½”ë“œ, í–‰ì •ë™ì½”ë“œ ë³„ë¡œ ì—…ì†Œìˆ˜
#   2. ë¶„ê¸°ë³€í™”ì— ë”°ë¥¸ ì—…ì²´ìˆ˜ ìœ ì§€ìœ¨(ì „ë¶„ê¸°ì—…ì†ŒëŒ€ë¹„ ë‚¨ì•„ìˆëŠ” ì—…ì²´ìˆ˜)
#   3. ë¶„ê¸°ë³€í™”ì— ë”°ë¥¸ ì—…ì†Œìˆ˜ ë³€í™”ëŸ‰(ì „ë¶„ê¸°ì—…ì†Œìˆ˜ëŒ€ë¹„ ì¦ê°€/ê°ì†Œí•œ ì—…ì†Œìˆ˜)
# [[Part2. ë°ì´í„°í”„ë ˆì„ì„ ìµœì¢… ë³‘í•©í•œë‹¤.]]
#   - ì„ëŒ€ë£Œ, ì°©í•œê°€ê²©ì—…ì†Œ_ìš”ì•½, ë§¤ì¶œì•¡, ì í¬ìˆ˜ ë“± í”„ë ˆì„ ë³‘í•©
# [[Part3. ë§¤ì¶œì•¡, ì í¬ìˆ˜, ì„ëŒ€ë£Œ ë³€í™”ëŸ‰]] - ë¬¼ê°€ëŒ€ë¦¬ë³€ìˆ˜
#   - ì„ëŒ€ë£Œ, ë§¤ì¶œì•¡, ì í¬ìˆ˜ ì •ê·œí™” 
#   - Resional Price Proxy(RPP) = ì„ëŒ€ë£Œ_norm*0.6 + ë§¤ì¶œì•¡/ì í¬ìˆ˜_norm*0.1
# ===============================================================================

# ---------------------------------------
# Part1. ì°©í•œê°€ê²©ì—…ì†Œ íŒŒìƒë³€ìˆ˜ ìƒì„± 
# ---------------------------------------
GoodPrices = pd.read_csv('./data/ì°©í•œê°€ê²©ì—…ì†Œ.csv', encoding='utf-8')

GoodPrices[['ê¸°ì¤€_ë…„ë¶„ê¸°_ì½”ë“œ','í–‰ì •ë™_ëª…','í–‰ì •ë™_ì½”ë“œ']] = GoodPrices[['ê¸°ì¤€_ë…„ë¶„ê¸°_ì½”ë“œ','í–‰ì •ë™_ëª…','í–‰ì •ë™_ì½”ë“œ']].astype('str')
GoodPrices['í–‰ì •ë™_ì½”ë“œ'] = GoodPrices['í–‰ì •ë™_ì½”ë“œ'].str[:8]

# 1. ê¸°ì¤€_ë…„ë¶„ê¸°_ì½”ë“œ, í–‰ì •ë™ì½”ë“œ ë³„ ì—…ì†Œìˆ˜ ê³„ì‚°
shop_counts = GoodPrices.groupby(['ê¸°ì¤€_ë…„ë¶„ê¸°_ì½”ë“œ', 'í–‰ì •ë™_ì½”ë“œ','í–‰ì •ë™_ëª…'])['ì—…ì†Œëª…'].nunique().reset_index()
shop_counts.rename(columns={'ì—…ì†Œëª…': 'ì—…ì†Œìˆ˜'}, inplace=True)

# 2. ë¶„ê¸° ë³€í™”ì— ë”°ë¥¸ ìœ ì§€ìœ¨ ë° ì—…ì†Œìˆ˜ ì¦ê°ëŸ‰ ê³„ì‚°
quarters = sorted(GoodPrices['ê¸°ì¤€_ë…„ë¶„ê¸°_ì½”ë“œ'].unique())
records = []

# ì—…ì†Œëª… ë„ì–´ì“°ê¸° ë¶™ì´ê¸° 
def clean_name(name):
    if pd.isna(name):
        return ""
    return name.replace(" ", "").strip().lower()

# ì—…ì†Œëª…ì´ ì¼ë¶€ ë³€ê²½ëœ ê²½ìš°ë„ ìˆìœ¼ë¯€ë¡œ, ìœ ì‚¬ë„ ê¸°ë°˜ìœ¼ë¡œ ë§¤ì¹­í•œë‹¤. 
# ex) í‰ë²”ì‹ë‹¹ -> ì œì¼í‰ë²”ì‹ë‹¹ 
def fuzzy_match(name1, name2):
    return (name1 in name2 or 
            name2 in name1 or
            Levenshtein.ratio(name1, name2) >= 0.5
           )

# ê° ë¶„ê¸°ë³„ë¡œ ì „ë¶„ê¸°ëŒ€ë¹„ ì—…ì†Œìˆ˜ ì¦ê°, ì—…ì†Œì˜ ìœ ì§€ìœ¨(retension)ì„ êµ¬í•œë‹¤.
for i in range(1, len(quarters)):
    prev_q = quarters[i - 1]
    curr_q = quarters[i]

    df_prev = GoodPrices[GoodPrices['ê¸°ì¤€_ë…„ë¶„ê¸°_ì½”ë“œ'] == prev_q]
    df_curr = GoodPrices[GoodPrices['ê¸°ì¤€_ë…„ë¶„ê¸°_ì½”ë“œ'] == curr_q]

    all_dongs = set(df_prev['í–‰ì •ë™_ì½”ë“œ']) | set(df_curr['í–‰ì •ë™_ì½”ë“œ'])

    for dong in all_dongs:
        prev_names = df_prev[df_prev['í–‰ì •ë™_ì½”ë“œ'] == dong]['ì—…ì†Œëª…'].dropna().apply(clean_name).tolist()
        curr_names = df_curr[df_curr['í–‰ì •ë™_ì½”ë“œ'] == dong]['ì—…ì†Œëª…'].dropna().apply(clean_name).tolist()

        prev_count = len(prev_names)
        curr_count = len(curr_names)

        retained = set()
        for pname in prev_names:
            for cname in curr_names:
                if fuzzy_match(pname, cname):
                    retained.add(pname)
                    break  # í•˜ë‚˜ë¼ë„ ë§¤ì¹­ë˜ë©´ ê·¸ ì´ì „ ì´ë¦„ì€ ìœ ì§€ëœ ê²ƒìœ¼ë¡œ ì²˜ë¦¬

        retention_rate = len(retained) / prev_count if prev_count > 0 else None
        change_count = curr_count - prev_count if prev_count > 0 else None

        records.append({
            'ê¸°ì¤€_ë…„ë¶„ê¸°_ì½”ë“œ': curr_q,
            'í–‰ì •ë™_ì½”ë“œ': dong,
            'ì „ë¶„ê¸°ëŒ€ë¹„_ìœ ì§€ìœ¨': retention_rate,
            'ì „ë¶„ê¸°ëŒ€ë¹„_ì¦ê°ì—…ì†Œìˆ˜': change_count
        })

change_df = pd.DataFrame(records)

# ìµœì¢… ë³‘í•©
GoodPrices_summary = pd.merge(shop_counts, change_df, on=['ê¸°ì¤€_ë…„ë¶„ê¸°_ì½”ë“œ', 'í–‰ì •ë™_ì½”ë“œ'], how='left')
GoodPrices_summary.to_csv('ì°©í•œê°€ê²©ì—…ì†Œ_ìš”ì•½.csv',encoding='utf-8-sig', index=False)

# ------------------------------------------------------------------
# Part2. ì„ëŒ€ë£Œ, ì°©í•œê°€ê²©ì—…ì†Œ_ìš”ì•½, ë§¤ì¶œì•¡, ì í¬ìˆ˜ ë“±ì˜ ë°ì´í„°í”„ë ˆì„ì„ ìµœì¢… ë³‘í•©í•œë‹¤.
# -------------------------------------------------------------------
df_ìƒê¶Œë°ì´í„°= pd.read_csv('./data/ìƒê¶Œë°ì´í„°.csv', encoding='utf-8')
df_ì§€ì—­ë³„_ì„ëŒ€ë£Œ = pd.read_csv('./data/ì§€ì—­ë³„_ì„ëŒ€ë£Œ.csv', encoding='utf-8')
df_ì°©í•œê°€ê²©ì—…ì†Œ_ìš”ì•½ = pd.read_csv('./data/ì°©í•œê°€ê²©ì—…ì†Œ_ìš”ì•½.csv', encoding='utf-8')

# ì»¬ëŸ¼í˜•ì‹ ë³€ê²½ 
df_ìƒê¶Œë°ì´í„°[['ê¸°ì¤€_ë…„ë¶„ê¸°_ì½”ë“œ','í–‰ì •ë™_ì½”ë“œ']] = df_ìƒê¶Œë°ì´í„°[['ê¸°ì¤€_ë…„ë¶„ê¸°_ì½”ë“œ','í–‰ì •ë™_ì½”ë“œ']].astype('str')
df_ì§€ì—­ë³„_ì„ëŒ€ë£Œ[['ê¸°ì¤€_ë…„ë¶„ê¸°_ì½”ë“œ','í–‰ì •ë™_ì½”ë“œ']] = df_ì§€ì—­ë³„_ì„ëŒ€ë£Œ[['ê¸°ì¤€_ë…„ë¶„ê¸°_ì½”ë“œ','í–‰ì •ë™_ì½”ë“œ']].astype('str')
df_ì°©í•œê°€ê²©ì—…ì†Œ_ìš”ì•½[['ê¸°ì¤€_ë…„ë¶„ê¸°_ì½”ë“œ','í–‰ì •ë™_ì½”ë“œ']] = df_ì°©í•œê°€ê²©ì—…ì†Œ_ìš”ì•½[['ê¸°ì¤€_ë…„ë¶„ê¸°_ì½”ë“œ','í–‰ì •ë™_ì½”ë“œ']].astype('str')

# 2023~24ë…„ë„ ë°ì´í„°ë§Œ í•„í„°ë§
years = ('2023', '2024')

df_base_2023_2024 = df_ìƒê¶Œë°ì´í„°[df_ìƒê¶Œë°ì´í„°['ê¸°ì¤€_ë…„ë¶„ê¸°_ì½”ë“œ'].str.startswith(years)]
df_ì§€ì—­ë³„_ì„ëŒ€ë£Œ_2023_2024 = df_ì§€ì—­ë³„_ì„ëŒ€ë£Œ[df_ì§€ì—­ë³„_ì„ëŒ€ë£Œ['ê¸°ì¤€_ë…„ë¶„ê¸°_ì½”ë“œ'].str.startswith(years)]
df_ì°©í•œê°€ê²©ì—…ì†Œ_ìš”ì•½_2023_2024 = df_ì°©í•œê°€ê²©ì—…ì†Œ_ìš”ì•½[df_ì°©í•œê°€ê²©ì—…ì†Œ_ìš”ì•½['ê¸°ì¤€_ë…„ë¶„ê¸°_ì½”ë“œ'].str.startswith(years)]

# í–‰ì •ë™ì½”ë“œ í†µì¼ 
df_ì§€ì—­ë³„_ì„ëŒ€ë£Œ_2023_2024['í–‰ì •ë™_ì½”ë“œ'] = df_ì§€ì—­ë³„_ì„ëŒ€ë£Œ_2023_2024['í–‰ì •ë™_ì½”ë“œ'].str[:8]
df_ì°©í•œê°€ê²©ì—…ì†Œ_ìš”ì•½_2023_2024['í–‰ì •ë™_ì½”ë“œ'] = df_ì°©í•œê°€ê²©ì—…ì†Œ_ìš”ì•½_2023_2024['í–‰ì •ë™_ì½”ë“œ'].str[:8]

# data merge 
merge_keys = ['ê¸°ì¤€_ë…„ë¶„ê¸°_ì½”ë“œ','í–‰ì •ë™_ì½”ë“œ']

# ë¶„ì„ì„ ìœ„í•´ ì„ëŒ€ë£Œê°€ ìˆëŠ” ì§€ì—­ì„ base í…Œì´ë¸”ë¡œí•˜ì—¬ join í•œë‹¤.
df_GoodPrice = df_ì§€ì—­ë³„_ì„ëŒ€ë£Œ_2023_2024.merge(df_base_2023_2024, on=merge_keys, how='left') \
                                      .merge(df_ì°©í•œê°€ê²©ì—…ì†Œ_ìš”ì•½_2023_2024, on=merge_keys, how='left')

# version2
df_GoodPrice = df_base_2023_2024.merge(df_ì°©í•œê°€ê²©ì—…ì†Œ_ìš”ì•½_2023_2024, on=merge_keys, how='left')

# ì„ì‹œ ê²°ì¸¡ì¹˜ ì œê±° 
df_GoodPrice = df_GoodPrice[df_GoodPrice['ê¸°ì¤€_ë…„ë¶„ê¸°_ì½”ë“œ'] != '20245']
df_GoodPrice = df_GoodPrice[df_GoodPrice['í–‰ì •ë™_ì½”ë“œ']!='nan']
df_GoodPrice = df_GoodPrice[df_GoodPrice['í–‰ì •ë™'].notna()]

# cond_new(ì°©í•œê°€ê²©ì—…ì†Œ ì‹ ê·œë“±ì¥), cond_empty(ì°©í•œê°€ê²©ì—…ì†Œ ì—†ëŠ”ì§€ì—­)
cond_new = (df_GoodPrice['ì—…ì†Œìˆ˜'].notna()) & (df_GoodPrice['ì „ë¶„ê¸°ëŒ€ë¹„_ìœ ì§€ìœ¨'].isna())
cond_empty = (df_GoodPrice['ì—…ì†Œìˆ˜'].isna()) & (df_GoodPrice['ì „ë¶„ê¸°ëŒ€ë¹„_ìœ ì§€ìœ¨'].isna())

# 1. ì‹ ê·œ ì§„ì… êµ¬ì—­: ìœ ì§€ìœ¨ = 1, ì¦ê° = ì—…ì†Œìˆ˜
df_GoodPrice.loc[cond_new, 'ì „ë¶„ê¸°ëŒ€ë¹„_ìœ ì§€ìœ¨'] = 1
df_GoodPrice.loc[cond_new, 'ì „ë¶„ê¸°ëŒ€ë¹„_ì¦ê°ì—…ì†Œìˆ˜'] = df_GoodPrice.loc[cond_new, 'ì—…ì†Œìˆ˜']

# 2. ì™„ì „ ê³µë°± êµ¬ì—­: ìœ ì§€ìœ¨ = 0, ì¦ê° = 0
df_GoodPrice.loc[cond_empty, ['ì—…ì†Œìˆ˜', 'ì „ë¶„ê¸°ëŒ€ë¹„_ìœ ì§€ìœ¨', 'ì „ë¶„ê¸°ëŒ€ë¹„_ì¦ê°ì—…ì†Œìˆ˜']] = 0

# ì´_ìœ ë™ì¸êµ¬ 
df_GoodPrice['ì´_ìœ ë™ì¸êµ¬_ìˆ˜'] = df_GoodPrice['ë‚¨ì„±_ìœ ë™ì¸êµ¬_ìˆ˜'] + df_GoodPrice['ì—¬ì„±_ìœ ë™ì¸êµ¬_ìˆ˜'] 

# ì í¬ìˆ˜_ëŒ€ë¹„_ë§¤ì¶œì•¡ ìƒì„±
df_GoodPrice['ì í¬ìˆ˜_ëŒ€ë¹„_ë§¤ì¶œì•¡'] = df_GoodPrice.apply(
    lambda row: 0 if row['ì í¬_ìˆ˜'] == 0 or pd.isna(row['ì í¬_ìˆ˜'])
    else int(np.floor(row['ë‹¹ì›”_ë§¤ì¶œ_ê¸ˆì•¡'] / row['ì í¬_ìˆ˜'])),
    axis=1
)

# 10~30ëŒ€ ìœ ë™ì¸êµ¬ í•©ê³„ 
df_GoodPrice['ìœ ë™ì¸êµ¬_10_30ëŒ€'] = (
    df_GoodPrice['ì—°ë ¹ëŒ€_10_ìœ ë™ì¸êµ¬_ìˆ˜'] +
    df_GoodPrice['ì—°ë ¹ëŒ€_20_ìœ ë™ì¸êµ¬_ìˆ˜'] +
    df_GoodPrice['ì—°ë ¹ëŒ€_30_ìœ ë™ì¸êµ¬_ìˆ˜']
)

# 40~60ëŒ€ ì´ìƒ ìœ ë™ì¸êµ¬ìˆ˜ í•©ê³„
df_GoodPrice['ìœ ë™ì¸êµ¬_40_ì´ìƒ'] = (
    df_GoodPrice['ì—°ë ¹ëŒ€_40_ìœ ë™ì¸êµ¬_ìˆ˜'] +
    df_GoodPrice['ì—°ë ¹ëŒ€_50_ìœ ë™ì¸êµ¬_ìˆ˜'] +
    df_GoodPrice['ì—°ë ¹ëŒ€_60_ì´ìƒ_ìœ ë™ì¸êµ¬_ìˆ˜']
)

# ì´_ì§ì¥ì¸êµ¬
df_GoodPrice['ì´_ì§ì¥ì¸êµ¬_ìˆ˜'] = df_GoodPrice['ë‚¨ì„±_ì§ì¥_ì¸êµ¬_ìˆ˜'] + df_GoodPrice['ì—¬ì„±_ì§ì¥_ì¸êµ¬_ìˆ˜'] 
# ì§€ì—­ë³„ ì í¬ìˆ˜ ëŒ€ë¹„ ì—…ì†Œìˆ˜ 
df_GoodPrice['ì°©í•œê°€ê²©_ì—…ì†Œìˆ˜_ë¹„ì¤‘'] = (
    df_GoodPrice['ì—…ì†Œìˆ˜'] / df_GoodPrice['ì í¬_ìˆ˜']
).round(3)

# ìµœì¢…ë°ì´í„°ì…‹ export
df_GoodPrice.to_csv('./model/ìƒê¶Œ_ì°©í•œê°€ê²©ì—…ì†Œ_ë³‘í•©.csv',encoding='utf-8-sig', index=False)


#df_GoodPrice['log_ì„ëŒ€ë£Œ'] = np.log(df_GoodPrice['í‰ê· ì„ëŒ€ë£Œ'])
#df_GoodPrice['log_ì í¬ìˆ˜_ëŒ€ë¹„_ë§¤ì¶œì•¡'] = np.log(df_GoodPrice['ì í¬ìˆ˜_ëŒ€ë¹„_ë§¤ì¶œì•¡'])

#df_GoodPrice = zscore_scale(df_GoodPrice,'log_ì„ëŒ€ë£Œ')
#df_GoodPrice = zscore_scale(df_GoodPrice,'log_ì í¬ìˆ˜_ëŒ€ë¹„_ë§¤ì¶œì•¡')

#df_GoodPrice['ë¬¼ê°€_proxy'] = df_GoodPrice['log_ì„ëŒ€ë£Œ']*0.6 + df_GoodPrice['log_ì í¬ìˆ˜_ëŒ€ë¹„_ë§¤ì¶œì•¡']*0.4


# ===============================================================================
#  1. íšŒê·€_ë¶„ì„
#  [Model1]
#   H1. ì§€ì—­ ë‚´ ì™¸ì‹ì§€ì¶œë¹„ê°€ ë†’ì€ì§€ì—­ì¼ìˆ˜ë¡ ì°©í•œê°€ê²©ì—…ì†Œ ìˆ˜ ë¹„ì¤‘ì´ ê°ì†Œí•œë‹¤. - ê²€ì¦ 
#   H2. ì§€ì—­ ë‚´ íì—…ë¥ ì´ ë†’ì€ì§€ì—­ì¼ìˆ˜ë¡ ì°©í•œê°€ê²©ì—…ì†Œ ìˆ˜ ë¹„ì¤‘ì´ ì¦ê°€í•œë‹¤ - ê²€ì¦ 
#   H3. ì§€ì—­ ë‚´ ìƒê¶Œì¶•ì†Œ ì§€ì—­ì¼ìˆ˜ë¡ ì°©í•œê°€ê²©ì—…ì†Œ ìˆ˜ ë¹„ì¤‘ì´ ì¦ê°€í•œë‹¤. - ê²€ì¦
#   H4. ì§€ì—­ ë‚´ 20_30ëŒ€ ì¸êµ¬ë¹„ê°€ ë†’ì€ì§€ì—­ì¼ìˆ˜ë¡ ì°©í•œê°€ê²©ì—…ì†Œ ìˆ˜ ë¹„ì¤‘ì´ ê°ì†Œí•œë‹¤. - ê¸°ê° 
#   [Model2]
#   H6. ì§€ì—­ ë‚´ ìƒê¶Œì¶•ì†Œ ì§€ì—­ì— ë”°ë¼ 20_30ëŒ€ ì¸êµ¬ë¹„ê°€ ì°©í•œê°€ê²©ì—…ì†Œ ìˆ˜ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì´ ë‹¤ë¥¼ ê²ƒì´ë‹¤.
#   [Model3]
#   ì¶”ê°€. H2,H3,H4 ì˜ lag_1 ë…ë¦½ ì‹œì°¨ë³€ìˆ˜ë¥¼ í†µí•´ ì—­ì¸ê³¼ì„±ì— ëŒ€í•œ ê°•ê±´ì„± ê²€ì¦ 
# ===============================================================================

# íƒ€ì…ë³€í™˜ 
df_GoodPrice['ê¸°ì¤€_ë…„ë¶„ê¸°_ì½”ë“œ'] = df_GoodPrice['ê¸°ì¤€_ë…„ë¶„ê¸°_ì½”ë“œ'].astype(int)

# ì„ì‹œì½”ë“œ(20234~20244 ë¶„ê¸° í•œì •)
df_GoodPrice = df_GoodPrice[df_GoodPrice['ê¸°ì¤€_ë…„ë¶„ê¸°_ì½”ë“œ'].isin([20234, 20241, 20242, 20243, 20244])]

# / ëŒ€ì²´ - íšŒê·€ë¶„ì„ì—ì„œ ì¸ì‹ì˜¤ë¥˜  
# df_GoodPrice['ìƒê¶Œëª…']= df_GoodPrice['ìƒê¶Œëª…'].str.replace('/', '', regex=False)

# 1. ë°ì´í„° ì¸ë±ìŠ¤ ì„¤ì •
df_panel = df_GoodPrice.set_index(['í–‰ì •ë™_ì½”ë“œ', 'ê¸°ì¤€_ë…„ë¶„ê¸°_ì½”ë“œ']).sort_index()

# 2. ë”ë¯¸ ë³€ìˆ˜ ìƒì„±
time_dummies = pd.get_dummies(df_panel.reset_index()['ê¸°ì¤€_ë…„ë¶„ê¸°_ì½”ë“œ'], prefix='ë¶„ê¸°', drop_first=True)

# 3. ê¸°ì¡´ ë³€ìˆ˜ì™€ ë”ë¯¸ ë³‘í•©
df_model = pd.concat([
    df_panel.reset_index(drop=True),
    time_dummies
], axis=1)

# 4. ì¸ë±ìŠ¤ ì¬ì„¤ì •
df_model = df_model.set_index(df_panel.index).sort_index()

# 5. ì¹´í…Œê³ ë¦¬í˜• ë³€ìˆ˜ ì›í•« ì¸ì½”ë”©
df_model = pd.get_dummies(df_model, columns=['ìƒê¶Œ_ë³€í™”_ì§€í‘œ'], drop_first=False)
df_model = df_model.drop(columns=['ìƒê¶Œ_ë³€í™”_ì§€í‘œ_HH'])

# 6. ë‹¤ì¤‘ê³µì„ ì„± í™•ì¸ corr Matrix 

# ë‹¤ì¤‘ê³µì„ ì„± í™•ì¸ ëŒ€ìƒ ë³€ìˆ˜ ë¦¬ìŠ¤íŠ¸
cols = ['ì í¬ìˆ˜_ëŒ€ë¹„_ë§¤ì¶œì•¡', 'ì›”_í‰ê· _ì†Œë“_ê¸ˆì•¡', 'ìŒì‹_ì§€ì¶œ_ì´ê¸ˆì•¡','ì˜ë£Œë¹„_ì§€ì¶œ_ì´ê¸ˆì•¡','êµìœ¡_ì§€ì¶œ_ì´ê¸ˆì•¡',
        'ì•„íŒŒíŠ¸_ë‹¨ì§€_ìˆ˜', 'ì•„íŒŒíŠ¸_í‰ê· _ì‹œê°€', 'ì´_ìœ ë™ì¸êµ¬_ìˆ˜', 'ìœ ë™ì¸êµ¬_10_30ëŒ€', 'ìœ ë™ì¸êµ¬_40_ì´ìƒ', 'ì´_ì§ì¥ì¸êµ¬_ìˆ˜', 'ì´_ìƒì£¼ì¸êµ¬_ìˆ˜','ì§‘ê°ì‹œì„¤_ìˆ˜','ìš´ì˜_ì˜ì—…_ê°œì›”_ì°¨ì´','íì—…_ì˜ì—…_ê°œì›”_ì°¨ì´','ê°œì—…_ë¥ ','íì—…_ë¥ ',
        '1ì¸_ê°€êµ¬ë¹„','20_30_ì¸êµ¬ë¹„','31_50_ì¸êµ¬ë¹„','51_75_ì¸êµ¬ë¹„']

# corr_matrix
corr_matrix = df_model[cols].dropna().corr().round(2)

# íˆíŠ¸ë§µ ê·¸ë¦¬ê¸°
fig = px.imshow(
    corr_matrix,
    text_auto=True,
    color_continuous_scale='RdBu_r',
    aspect='auto',
    title='ğŸ“Š ë³€ìˆ˜ ê°„ ìƒê´€ê³„ìˆ˜ íˆíŠ¸ë§µ (Plotly)'
)

fig.update_layout(
    width=800,
    height=700,
    margin=dict(l=50, r=50, t=50, b=50),
    coloraxis_colorbar=dict(title="ìƒê´€ê³„ìˆ˜")
)

fig.show()

# 7. VIF í™•ì¸ 
vif_cols = ['ì í¬ìˆ˜_ëŒ€ë¹„_ë§¤ì¶œì•¡', 'ì•„íŒŒíŠ¸_í‰ê· _ì‹œê°€', 'ìŒì‹_ì§€ì¶œ_ì´ê¸ˆì•¡','ì˜ë£Œë¹„_ì§€ì¶œ_ì´ê¸ˆì•¡','êµìœ¡_ì§€ì¶œ_ì´ê¸ˆì•¡',
            'ì•„íŒŒíŠ¸_ë‹¨ì§€_ìˆ˜', 'ì´_ìœ ë™ì¸êµ¬_ìˆ˜','ìœ ë™ì¸êµ¬_10_30ëŒ€','ìœ ë™ì¸êµ¬_40_ì´ìƒ', 'ì´_ì§ì¥ì¸êµ¬_ìˆ˜', 'ì´_ìƒì£¼ì¸êµ¬_ìˆ˜','ì§‘ê°ì‹œì„¤_ìˆ˜','ìš´ì˜_ì˜ì—…_ê°œì›”_ì°¨ì´','íì—…_ì˜ì—…_ê°œì›”_ì°¨ì´','ê°œì—…_ë¥ ','íì—…_ë¥ ',
            '1ì¸_ê°€êµ¬ë¹„','20_30_ì¸êµ¬ë¹„','31_50_ì¸êµ¬ë¹„','51_75_ì¸êµ¬ë¹„']

X = add_constant(df_model[vif_cols].dropna())
bool_cols = X.select_dtypes(include=['bool']).columns
X[bool_cols] = X[bool_cols].astype(float)

# VIF ê³„ì‚°
vif_data = pd.DataFrame()
vif_data['ë³€ìˆ˜'] = X.columns
vif_data['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]

# corr í° ì›”_í‰ê· ì†Œë“, ì§‘ê°ì‹œì„¤_ìˆ˜, ìš´ì˜_ì˜ì—…_ê°œì›”_ì°¨ì´ ì œê±°
print(vif_data)

# 8. ì™œë„ í™•ì¸ & ë¡œê·¸ë³€í™˜ 
skew_test_columns = [
    'ì—…ì†Œìˆ˜',
    'ì°©í•œê°€ê²©_ì—…ì†Œìˆ˜_ë¹„ì¤‘',
    'ì í¬ìˆ˜_ëŒ€ë¹„_ë§¤ì¶œì•¡',
    'ì›”_í‰ê· _ì†Œë“_ê¸ˆì•¡',
    'ìŒì‹_ì§€ì¶œ_ì´ê¸ˆì•¡',
    'ì˜ë£Œë¹„_ì§€ì¶œ_ì´ê¸ˆì•¡',
    'êµìœ¡_ì§€ì¶œ_ì´ê¸ˆì•¡',
    'ì•„íŒŒíŠ¸_ë‹¨ì§€_ìˆ˜',
    'ì•„íŒŒíŠ¸_í‰ê· _ì‹œê°€',
    'ì´_ìœ ë™ì¸êµ¬_ìˆ˜',
    'ìœ ë™ì¸êµ¬_10_30ëŒ€',
    'ìœ ë™ì¸êµ¬_40_ì´ìƒ',
    'ì´_ì§ì¥ì¸êµ¬_ìˆ˜',
    'ì´_ìƒì£¼ì¸êµ¬_ìˆ˜',
    'ì§‘ê°ì‹œì„¤_ìˆ˜',
    'ìš´ì˜_ì˜ì—…_ê°œì›”_ì°¨ì´',
    'íì—…_ì˜ì—…_ê°œì›”_ì°¨ì´',
    'ê°œì—…_ë¥ ',
    'íì—…_ë¥ ',
    '1ì¸_ê°€êµ¬ë¹„',
    '20_30_ì¸êµ¬ë¹„',
    '31_50_ì¸êµ¬ë¹„',
    '51_75_ì¸êµ¬ë¹„'
]

check_variable_skewness(df_model[skew_test_columns])

skew_columns = skew_test_columns.copy()
skew_columns.remove('ì´_ìƒì£¼ì¸êµ¬_ìˆ˜')       # ìƒì£¼ì¸êµ¬ìˆ˜ ëŒ€ì¹­ 
skew_columns.remove('ê°œì—…_ë¥ ')            # ê°œì—…ìœ¨
skew_columns.remove('íì—…_ë¥ ')            # íì—…ìœ¨
skew_columns.remove('íì—…_ì˜ì—…_ê°œì›”_ì°¨ì´')
skew_columns.remove('ìš´ì˜_ì˜ì—…_ê°œì›”_ì°¨ì´')

# ìµœì¢…í™•ì •ëœ ë³€ìˆ˜ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë¡œê·¸ë³€í™˜ 
df_model = apply_log_transform(df_model, skew_columns)

# 9. ë…ë¦½ë³€ìˆ˜ ìµœì¢…í™•ì • 
# - ì•„íŒŒíŠ¸_í‰ê· _ì‹œê°€_log, ì§‘ê°ì‹œì„¤ ìˆ˜, ìš´ì˜_ì˜ì—…_ê°œì›”_ì°¨ì´ëŠ” ë‹¤ì¤‘ê³µì„ ì„±ì´ ë†’ì•„ ì œê±°
ind_columns = [
    'ì í¬ìˆ˜_ëŒ€ë¹„_ë§¤ì¶œì•¡_log',
    'ìŒì‹_ì§€ì¶œ_ì´ê¸ˆì•¡_log',
    'ì˜ë£Œë¹„_ì§€ì¶œ_ì´ê¸ˆì•¡_log',
    'êµìœ¡_ì§€ì¶œ_ì´ê¸ˆì•¡_log',
    'ì•„íŒŒíŠ¸_í‰ê· _ì‹œê°€_log',
    'ì•„íŒŒíŠ¸_ë‹¨ì§€_ìˆ˜_log',
    'ìœ ë™ì¸êµ¬_10_30ëŒ€_log',
    'ìœ ë™ì¸êµ¬_40_ì´ìƒ_log',
    'ì´_ì§ì¥ì¸êµ¬_ìˆ˜_log',
    'ì´_ìƒì£¼ì¸êµ¬_ìˆ˜',
    'ì§‘ê°ì‹œì„¤_ìˆ˜_log',
    'ìƒê¶Œ_ë³€í™”_ì§€í‘œ_LL',
    'ìƒê¶Œ_ë³€í™”_ì§€í‘œ_HL',
    'ìƒê¶Œ_ë³€í™”_ì§€í‘œ_LH',
    'ê°œì—…_ë¥ ',
    'íì—…_ë¥ ',
    '1ì¸_ê°€êµ¬ë¹„_log',
    '20_30_ì¸êµ¬ë¹„_log',
    '31_50_ì¸êµ¬ë¹„_log'
]
dep_columns= ['ì—…ì†Œìˆ˜_log','ì°©í•œê°€ê²©_ì—…ì†Œìˆ˜_ë¹„ì¤‘','ì°©í•œê°€ê²©_ì—…ì†Œìˆ˜_ë¹„ì¤‘_log']
dummy_columns= ['ë¶„ê¸°_20241','ë¶„ê¸°_20242','ë¶„ê¸°_20243','ë¶„ê¸°_20244']

# 10. ì´ìƒì¹˜ íŒŒì•… í›„ ì œê±° 
check_outliers_std(df_model[ind_columns],3.0)
df_model_drop_outlier = drop_outlier_rows_std(df_model, ind_columns)

scale_columns = ind_columns.copy()
scale_columns.remove('ìƒê¶Œ_ë³€í™”_ì§€í‘œ_LL')       
scale_columns.remove('ìƒê¶Œ_ë³€í™”_ì§€í‘œ_HL')            
scale_columns.remove('ìƒê¶Œ_ë³€í™”_ì§€í‘œ_LH')            

# 11. ë…ë¦½ë³€ìˆ˜ ë‹¨ìœ„ ìŠ¤ì¼€ì¼ë§
df_model_after_scaling = apply_zscore_scaling(df_model_drop_outlier,scale_columns)

selected_columns = ind_columns + dep_columns + dummy_columns
df_final = df_model_after_scaling[selected_columns].copy()
df_final.columns = df_final.columns.str.strip()

# -----------------------------------
# 12. Model1 íšŒê·€ì‹ êµ¬ì„± (ê°€ì„¤1,2,3,4,5)
# -----------------------------------
base_formula = 'ì°©í•œê°€ê²©_ì—…ì†Œìˆ˜_ë¹„ì¤‘_log ~ 1 + ì í¬ìˆ˜_ëŒ€ë¹„_ë§¤ì¶œì•¡_log + ìƒê¶Œ_ë³€í™”_ì§€í‘œ_HL + ìƒê¶Œ_ë³€í™”_ì§€í‘œ_LH + ìƒê¶Œ_ë³€í™”_ì§€í‘œ_LL + íì—…_ë¥  + ìŒì‹_ì§€ì¶œ_ì´ê¸ˆì•¡_log + ì•„íŒŒíŠ¸_í‰ê· _ì‹œê°€_log + ì•„íŒŒíŠ¸_ë‹¨ì§€_ìˆ˜_log + ìœ ë™ì¸êµ¬_10_30ëŒ€_log + ìœ ë™ì¸êµ¬_40_ì´ìƒ_log + ì´_ì§ì¥ì¸êµ¬_ìˆ˜_log + ì§‘ê°ì‹œì„¤_ìˆ˜_log + ì´_ìƒì£¼ì¸êµ¬_ìˆ˜ + 1ì¸_ê°€êµ¬ë¹„_log + 20_30_ì¸êµ¬ë¹„_log + 31_50_ì¸êµ¬ë¹„_log'
dummy_formula = ' + '.join(time_dummies.columns.tolist())
full_formula = base_formula + ' + ' + dummy_formula

# PanelOLS ì í•©
model = PanelOLS.from_formula(full_formula, data=df_final)
result = model.fit()
print(result.summary)

# rmsle
rmsle = compute_rmsle_from_result(result, df_final)

# ëª¨ë¸1 ê²°ê³¼ ì €ì¥ 
save_full_model_output(result,rmsle,"./model/model1_results.csv")

# --------------------------------------------------------------------------
# 13. Model2 íšŒê·€ì‹ êµ¬ì„± (ê°€ì„¤6) - (ì¡°ì ˆë³€ìˆ˜ - ìƒí˜¸ì‘ìš© í•­) ì°©í•œê°€ê²©ì—…ì†Œìˆ˜ ë¹„ì¤‘ì˜ ì¶”ê°€ ì¦/ê° ê²€ì¦
# --------------------------------------------------------------------------
base_formula2 = 'ì°©í•œê°€ê²©_ì—…ì†Œìˆ˜_ë¹„ì¤‘_log ~ 1 + ì í¬ìˆ˜_ëŒ€ë¹„_ë§¤ì¶œì•¡_log + ìƒê¶Œ_ë³€í™”_ì§€í‘œ_HL + ìƒê¶Œ_ë³€í™”_ì§€í‘œ_LH + ìƒê¶Œ_ë³€í™”_ì§€í‘œ_LL + íì—…_ë¥  + ìŒì‹_ì§€ì¶œ_ì´ê¸ˆì•¡_log + ì•„íŒŒíŠ¸_í‰ê· _ì‹œê°€_log + ì•„íŒŒíŠ¸_ë‹¨ì§€_ìˆ˜_log + ì´_ìœ ë™ì¸êµ¬_ìˆ˜_log + ì´_ì§ì¥ì¸êµ¬_ìˆ˜_log + ì§‘ê°ì‹œì„¤_ìˆ˜_log + ì´_ìƒì£¼ì¸êµ¬_ìˆ˜ + 1ì¸_ê°€êµ¬ë¹„_log + 20_30_ì¸êµ¬ë¹„_log + ìƒê¶Œ_ë³€í™”_ì§€í‘œ_HL:20_30_ì¸êµ¬ë¹„_log + ìƒê¶Œ_ë³€í™”_ì§€í‘œ_LH:20_30_ì¸êµ¬ë¹„_log + ìƒê¶Œ_ë³€í™”_ì§€í‘œ_LL:20_30_ì¸êµ¬ë¹„_log + 31_50_ì¸êµ¬ë¹„_log'
dummy_formula2 = ' + '.join(time_dummies.columns.tolist())
full_formula2 = base_formula2 + ' + ' + dummy_formula2

# PanelOLS ì í•©
model2 = PanelOLS.from_formula(full_formula2, data=df_final)
result2 = model2.fit()
print(result2.summary)

# rmsle
rmsle2 = compute_rmsle_from_result(result2, df_final)

# ëª¨ë¸2 ê²°ê³¼ ì €ì¥ 
save_full_model_output(result2,rmsle2,"./model/model2_results.csv")

# -----------------------------
# 14. Model3 ì‹œì°¨ë³€ìˆ˜ ì¶”ê°€ í›„ ì¬ê²€ì¦ 
# -----------------------------
df_final = df_final.sort_values(by=['í–‰ì •ë™_ì½”ë“œ', 'ê¸°ì¤€_ë…„ë¶„ê¸°_ì½”ë“œ'])

# ì‹œì°¨ ìƒì„± ëŒ€ìƒ ë³€ìˆ˜ ë¦¬ìŠ¤íŠ¸
lag_vars = [
    'íì—…_ë¥ ',
    'ìŒì‹_ì§€ì¶œ_ì´ê¸ˆì•¡_log',
    'ìƒê¶Œ_ë³€í™”_ì§€í‘œ_HL',
    'ìƒê¶Œ_ë³€í™”_ì§€í‘œ_LH',
    'ìƒê¶Œ_ë³€í™”_ì§€í‘œ_LL',
    '20_30_ì¸êµ¬ë¹„_log'
]

# ê° ë³€ìˆ˜ì— ëŒ€í•´ -1ë¶„ê¸° ì‹œì°¨ ìƒì„±
for var in lag_vars:
    df_final[f'{var}_lag1'] = df_final.groupby('í–‰ì •ë™_ì½”ë“œ')[var].shift(1)

# ì²« ë¶„ê¸° ì œê±°(lag1 ê°’ì´ null ì¸ ë¶„ê¸°)
first_quarter_idx = df_final.groupby('í–‰ì •ë™_ì½”ë“œ').head(1).index
df_lagged = df_final.drop(index=first_quarter_idx)

# íšŒê·€ì‹ êµ¬ì„± (ê°€ì„¤1, 2) - ì‹œì°¨ë³€ìˆ˜
lag_time_dummies = pd.get_dummies(df_lagged.reset_index()['ê¸°ì¤€_ë…„ë¶„ê¸°_ì½”ë“œ'], prefix='ë¶„ê¸°', drop_first=True)

base_formula3 = 'ì°©í•œê°€ê²©_ì—…ì†Œìˆ˜_ë¹„ì¤‘_log ~ 1 + ì í¬ìˆ˜_ëŒ€ë¹„_ë§¤ì¶œì•¡_log + ìƒê¶Œ_ë³€í™”_ì§€í‘œ_HL_lag1 + ìƒê¶Œ_ë³€í™”_ì§€í‘œ_LH_lag1 + ìƒê¶Œ_ë³€í™”_ì§€í‘œ_LL_lag1 + íì—…_ë¥ _lag1 + ìŒì‹_ì§€ì¶œ_ì´ê¸ˆì•¡_log_lag1 + ì•„íŒŒíŠ¸_í‰ê· _ì‹œê°€_log + ì•„íŒŒíŠ¸_ë‹¨ì§€_ìˆ˜_log + ìœ ë™ì¸êµ¬_10_30ëŒ€_log + ìœ ë™ì¸êµ¬_40_ì´ìƒ_log + ì´_ì§ì¥ì¸êµ¬_ìˆ˜_log + ì§‘ê°ì‹œì„¤_ìˆ˜_log + ì´_ìƒì£¼ì¸êµ¬_ìˆ˜ + 1ì¸_ê°€êµ¬ë¹„_log + 20_30_ì¸êµ¬ë¹„_log_lag1 + 31_50_ì¸êµ¬ë¹„_log'
dummy_formula3 = ' + '.join(lag_time_dummies.columns.tolist())
full_formula3 = base_formula3 + ' + ' + dummy_formula3

# PanelOLS ì í•©
model3 = PanelOLS.from_formula(full_formula3, data=df_lagged)
result3 = model3.fit()
print(result3.summary)

# rmsle
rmsle3 = compute_rmsle_from_result(result3, df_final)

# ëª¨ë¸3 ê²°ê³¼ ì €ì¥ 
save_full_model_output(result3,rmsle3,"./model/model3_results.csv")

# ===========================================================
# 4. ìœ ì˜í•œ ë³€ìˆ˜ë¥¼ í†µí•´ clustering 
#  - ê³µí†µì „ì²˜ë¦¬(ìŠ¤ì¼€ì¼ë§, SPCA) 
#  [Part1. ê³„ì¸µì  Clustering]
#  [Part2. K-means í´ëŸ¬ìŠ¤í„°ë§]
#  [Part3. ë³‘í•© ë° í´ëŸ¬ìŠ¤í„°ë§ ëª…ì¹­ë¶€ì—¬]
#  [Part4. ë¹„êµì‹œê°í™”]
# ===========================================================
df_for_cluster = pd.read_csv('./model/ìƒê¶Œ_ì°©í•œê°€ê²©ì—…ì†Œ_ë³‘í•©.csv', encoding='utf-8')

# ì°©í•œê°€ê²©_ì—…ì†Œìˆ˜_ë¹„ì¤‘
df_for_cluster['ì°©í•œê°€ê²©_ì—…ì†Œìˆ˜_ë¹„ì¤‘'] = (
    df_for_cluster['ì—…ì†Œìˆ˜'] / df_for_cluster['ì í¬_ìˆ˜']
).round(3)

# ë”ë¯¸ë³€ìˆ˜ ìƒì„±
df_for_cluster = pd.get_dummies(df_for_cluster, columns=['ìƒê¶Œ_ë³€í™”_ì§€í‘œ'], drop_first=False)

# ì™œë„íŒŒì•… 
skew_columns = ['ì°©í•œê°€ê²©_ì—…ì†Œìˆ˜_ë¹„ì¤‘','ìŒì‹_ì§€ì¶œ_ì´ê¸ˆì•¡','íì—…_ë¥ ','20_30_ì¸êµ¬ë¹„']
scale_columns = ['ìŒì‹_ì§€ì¶œ_ì´ê¸ˆì•¡_log','íì—…_ë¥ _log','20_30_ì¸êµ¬ë¹„_log']
check_variable_skewness(df_for_cluster[skew_columns])

# ìµœì¢…í™•ì •ëœ ë³€ìˆ˜ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë¡œê·¸ë³€í™˜ 
df_for_cluster = apply_log_transform(df_for_cluster, skew_columns)

# ë…ë¦½ë³€ìˆ˜ ë‹¨ìœ„ ìŠ¤ì¼€ì¼ë§
df_for_cluster = apply_zscore_scaling(df_for_cluster, scale_columns)

# â–¶ ìœ ì˜ë³€ìˆ˜ ê¸°ë°˜ í´ëŸ¬ìŠ¤í„°ë§ìš© ë°ì´í„° ì¶”ì¶œ
features = ['ìŒì‹_ì§€ì¶œ_ì´ê¸ˆì•¡_log', 'íì—…_ë¥ _log', '20_30_ì¸êµ¬ë¹„_log','ìƒê¶Œ_ë³€í™”_ì§€í‘œ_HL','ìƒê¶Œ_ë³€í™”_ì§€í‘œ_LH','ìƒê¶Œ_ë³€í™”_ì§€í‘œ_LL']
y_var = 'ì°©í•œê°€ê²©_ì—…ì†Œìˆ˜_ë¹„ì¤‘'
df_spca = df_for_cluster[[y_var]+features].dropna().copy()

# â–¶ ìŠ¤ì¼€ì¼ë§ (ì—°ì†í˜•ë§Œ)
#scale_vars = ['íì—…_ë¥ ', 'ìŒì‹_ì§€ì¶œ_ì´ê¸ˆì•¡', '20_30_ì¸êµ¬ë¹„']
#dummy_vars = ['ìƒê¶Œ_ë³€í™”_ì§€í‘œ_HL']
#scaler = StandardScaler()
#X_scaled = scaler.fit_transform(df_spca[scale_vars])
#X_dummy = df_spca[['ìƒê¶Œ_ë³€í™”_ì§€í‘œ_HL']].astype(float).values

X_final = df_spca[features]
y = df_spca[y_var]

# â–¶ PCA 2ì°¨ì› ì¶•ì†Œ
pls = PLSRegression(n_components=2)
X_pls = pls.fit_transform(X_final, y)[0]  # ì£¼ì„±ë¶„ ì ìˆ˜ë§Œ ì¶”ì¶œ

# â–¶ ê³µí†µ ê²°ê³¼ ì €ì¥ìš© DF
df_pls_clustered = pd.DataFrame(X_pls, columns=['SPC1', 'SPC2'], index=df_spca.index)
df_pls_analysis= pd.DataFrame(pls.x_weights_, columns=['SPC1', 'SPC2'], index=features[:pls.x_weights_.shape[0]])

df_pls_analysis.index.name = "feature"  # ì¸ë±ìŠ¤ ì´ë¦„ ì„¤ì •
df_pls_analysis.to_csv("./model/pls_results.csv", index=True, encoding="utf-8-sig")

# ---------------------------------------
# [Part1. ê³„ì¸µì  Clustering]
# ---------------------------------------
Z = linkage(X_pls, method='ward')
hier_labels = fcluster(Z, t=4, criterion='maxclust')

df_pls_clustered['hier_cluster'] = hier_labels

# ---------------------------------------
# [Part2. K-means í´ëŸ¬ìŠ¤í„°ë§]
# ---------------------------------------
kmeans = KMeans(n_clusters=4, random_state=42, n_init='auto')
kmeans_labels = kmeans.fit_predict(X_pls)

df_pls_clustered['kmeans_cluster'] = kmeans_labels

# ---------------------------------------
# [Part3. ë³‘í•© ë° í´ëŸ¬ìŠ¤í„°ë§ ëª…ì¹­ë¶€ì—¬]
# ---------------------------------------
# ì¸ë±ìŠ¤ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë³‘í•© (ë‘˜ ë‹¤ ë™ì¼í•œ ìˆœì„œì¼ ê²½ìš°)
df_cluster = df_for_cluster.join(df_pls_clustered)

# SPC1, SPC2 ì¶• ì´ë¦„ ì¬ì •ì˜ (ì˜ˆ: ì†Œë¹„ì¶•, ì Šì€ì¸µì¶• ë“±)
df_cluster = df_cluster.rename(columns={
    'SPC1': 'ì†Œë¹„í™œì„±ë„_ì¶•',
    'SPC2': '2030_ì†Œë¹„ì ˆì œ_ì¶•'
})

# í´ëŸ¬ìŠ¤í„° ë²ˆí˜¸ì— ë”°ë¥¸ ëª…ì¹­ ë§¤í•‘ ë”•ì…”ë„ˆë¦¬
hier_cluster_labels = {
    1: 'ì „ì—°ë ¹_ê·¹_ì €ì†Œë¹„ì§€ì—­',
    2: 'ì „ì—°ë ¹_ì†Œë¹„_ë¹„í™œì„±ì§€ì—­',
    3: 'ì¤‘ì¥ë…„_ê³ ì†Œë¹„ì§€ì—­',
    4: 'ì²­ë…„_ê³ ì†Œë¹„ì§€ì—­',
}

# í´ëŸ¬ìŠ¤í„° ë²ˆí˜¸ì— ë”°ë¥¸ ëª…ì¹­ ë§¤í•‘ ë”•ì…”ë„ˆë¦¬
kmeans_cluster_labels = {
    0: 'ì „ì—°ë ¹_ì†Œë¹„_ë¹„í™œì„±ì§€ì—­',
    1: 'ì¤‘ì¥ë…„_ê³ ì†Œë¹„ì§€ì—­',
    2: 'ì²­ë…„_ê³ ì†Œë¹„ì§€ì—­',
    3: 'ì „ì—°ë ¹_ê·¹_ì €ì†Œë¹„ì§€ì—­'
}

# ìƒˆë¡œìš´ ì»¬ëŸ¼ ìƒì„± (ê¸°ì¡´ ìˆ«ì í´ëŸ¬ìŠ¤í„° ìœ ì§€ë„ ê°€ëŠ¥)
df_cluster['hier_cluster_label'] = df_cluster['hier_cluster'].map(hier_cluster_labels)
df_cluster['kmeans_cluster_label'] = df_cluster['kmeans_cluster'].map(kmeans_cluster_labels)

# ìµœì¢… íšŒê·€ ë°ì´í„° ì…‹ 
df_cluster.to_csv('./model/final_cluster.csv',encoding='utf-8-sig', index=False)

# ---------------------------------------
# [Part4. ë¹„êµì‹œê°í™”]
# ---------------------------------------

# â–¶ ê³„ì¸µì  í´ëŸ¬ìŠ¤í„°ë§ ê²°ê³¼
fig_hier = px.scatter(
    df_cluster,
    x='ì†Œë¹„í™œì„±ë„_ì¶•',
    y='2030_ì†Œë¹„ì ˆì œ_ì¶•',
    color=df_cluster['hier_cluster_label'].astype(str),
    title='[ê³„ì¸µì  í´ëŸ¬ìŠ¤í„°ë§] Supervised PCA ê¸°ë°˜ êµ°ì§‘ ê²°ê³¼',
    labels={'hier_cluster_label': 'Cluster'},
    hover_data={'ì†Œë¹„í™œì„±ë„_ì¶•': ':.2f', '2030_ì†Œë¹„ì ˆì œ_ì¶•': ':.2f'}
)
fig_hier.show()

# â–¶ KMeans ê²°ê³¼
fig_kmeans = px.scatter(
    df_cluster,
    x='ì†Œë¹„í™œì„±ë„_ì¶•',
    y='2030_ì†Œë¹„ì ˆì œ_ì¶•',
    color=df_cluster['kmeans_cluster_label'].astype(str),
    title='[K-Means í´ëŸ¬ìŠ¤í„°ë§] Supervised PCA ê¸°ë°˜ êµ°ì§‘ ê²°ê³¼',
    labels={'kmeans_cluster_label': 'Cluster'},
    hover_data={'ì†Œë¹„í™œì„±ë„_ì¶•': ':.2f', '2030_ì†Œë¹„ì ˆì œ_ì¶•': ':.2f'}
)
fig_kmeans.show()


# ===========================================================
# 5. ì‹œê°í™” ë° clustering ì „ëµì œì‹œ 
#   [Part1. ê·¸ë˜í”„ ì‹œê°í™”]
#   - ì‹œê°í™”1. ì—°ë„ë³„ ì°©í•œê°€ê²©ì—…ì†Œ ì¦ê°€ì¶”ì´ 
#   - ì‹œê°í™”2. í´ëŸ¬ìŠ¤í„°ë§ ìƒê¶Œë³„ ì°©í•œê°€ê²©ì—…ì†Œìˆ˜ ë¹„ìœ¨ (íŒŒì´ì°¨íŠ¸)
#   - ì‹œê°í™”3. í´ëŸ¬ìŠ¤í„°ë§ ìƒê¶Œë³„ ì°©í•œê°€ê²©ì—…ì†Œ ì¦ê°€ì¶”ì´ (ì„ ê·¸ë˜í”„ ì°¨íŠ¸)
#   
#   [Part2. ì§€ë„ ì‹œê°í™”]
#   - ì‹œê°í™”1. ì°©í•œê°€ê²©ì—…ì†Œ ë¶„í¬ ì  ì‹œê°í™” (mapbox)
#   - ì‹œê°í™”2. í´ëŸ¬ìŠ¤í„°ë§(k-means, 4ê°œ) - ì°©í•œê°€ê²©ì—…ì†Œìˆ˜ ì‹œê°í™” (mapbox)
# ===========================================================

df_ìƒê¶Œ_ì°©í•œê°€ê²©ì—…ì†Œ_ë³‘í•© = pd.read_csv('./model/ìƒê¶Œ_ì°©í•œê°€ê²©ì—…ì†Œ_ë³‘í•©.csv', encoding='utf-8')

# ------------------------------------------------------------------
#   [Part1. ê·¸ë˜í”„ ì‹œê°í™”]
#   - ì‹œê°í™”1. ì—°ë„ë³„ ì°©í•œê°€ê²©ì—…ì†Œ ì¦ê°€ì¶”ì´ 
#   - ì‹œê°í™”2. í´ëŸ¬ìŠ¤í„°ë§ ìƒê¶Œë³„ ì°©í•œê°€ê²©ì—…ì†Œìˆ˜ ë¹„ìœ¨ (íŒŒì´ì°¨íŠ¸)
#   - ì‹œê°í™”3. í´ëŸ¬ìŠ¤í„°ë§ ìƒê¶Œë³„ ì°©í•œê°€ê²©ì—…ì†Œ ì¦ê°€ì¶”ì´ (ì„ ê·¸ë˜í”„ ì°¨íŠ¸)
# ------------------------------------------------------------------
# ----------------------------
# ì‹œê°í™”1. ì—°ë„ë³„ ì°©í•œê°€ê²©ì—…ì†Œ ì¦ê°€ì¶”ì´ 
# ----------------------------
# ê¸°ì¤€_ë…„ë¶„ê¸°_ì½”ë“œë³„ ì „ì²´ ì—…ì†Œìˆ˜ ì´í•© ì§‘ê³„
df_trend = df_ìƒê¶Œ_ì°©í•œê°€ê²©ì—…ì†Œ_ë³‘í•©.groupby('ê¸°ì¤€_ë…„ë¶„ê¸°_ì½”ë“œ')['ì—…ì†Œìˆ˜'].sum().reset_index()
df_trend['ê¸°ì¤€_ë…„ë¶„ê¸°_ì½”ë“œ'] = df_trend['ê¸°ì¤€_ë…„ë¶„ê¸°_ì½”ë“œ'].astype('str')

# ì‹œê³„ì—´ ë¼ì¸ ì°¨íŠ¸ ìƒì„±
fig = px.line(
    df_trend,
    x='ê¸°ì¤€_ë…„ë¶„ê¸°_ì½”ë“œ',
    y='ì—…ì†Œìˆ˜',
    title='ê¸°ì¤€ ë¶„ê¸°ë³„ ì°©í•œê°€ê²©ì—…ì†Œ ìˆ˜ ë³€í™”',
    labels={'ê¸°ì¤€_ë…„ë¶„ê¸°_ì½”ë“œ': 'ê¸°ì¤€ ë¶„ê¸°', 'ì—…ì†Œìˆ˜': 'ì´ ì—…ì†Œìˆ˜'},
    markers=True
)

fig.update_layout(
    xaxis_title="ê¸°ì¤€_ë…„ë¶„ê¸°_ì½”ë“œ",
    yaxis_title="ì—…ì†Œìˆ˜",
    template='plotly_white',
    hovermode='x unified'
)

fig.show()

# --------------------------------------------
# ì‹œê°í™”2. í´ëŸ¬ìŠ¤í„°ë§ ìƒê¶Œë³„ ì°©í•œê°€ê²©ì—…ì†Œìˆ˜ ë¹„ìœ¨ (íŒŒì´ì°¨íŠ¸)
# --------------------------------------------
df_final_cluster = pd.read_csv('./model/final_cluster.csv', encoding='utf-8')

df_final_cluster.info()
# í´ëŸ¬ìŠ¤í„° ê¸°ì¤€ ì„ íƒ ('hc_cluster' ë˜ëŠ” 'kmeans_cluster')
cluster_col = 'hier_cluster_label'  # ë˜ëŠ” 'hc_cluster'

# ì—…ì†Œìˆ˜ ì§‘ê³„
df_pie_hc_cluster = df_final_cluster.groupby(cluster_col)[['ì í¬_ìˆ˜','ì—…ì†Œìˆ˜']].mean().reset_index()
df_pie_hc_cluster['ì°©í•œê°€ê²©_ì—…ì†Œìˆ˜_ë¹„ì¤‘'] = (
    df_pie_hc_cluster['ì—…ì†Œìˆ˜'] / df_pie_hc_cluster['ì í¬_ìˆ˜']
).round(3)


# íŒŒì´ì°¨íŠ¸ ìƒì„±
fig = px.pie(
    df_pie_hc_cluster,
    values='ì°©í•œê°€ê²©_ì—…ì†Œìˆ˜_ë¹„ì¤‘',
    names=cluster_col,
    title='í´ëŸ¬ìŠ¤í„°ë³„ ì—…ì†Œìˆ˜ ë¹„ì¤‘',
    hole=0.4  # ë„ë„›í˜•ìœ¼ë¡œ (ì„ íƒì‚¬í•­)
)

fig.update_traces(textinfo='percent+label')
fig.show()

# --------------------------------------------
# ì‹œê°í™”3. í´ëŸ¬ìŠ¤í„°ë§ ìƒê¶Œë³„ ì°©í•œê°€ê²©ì—…ì†Œìˆ˜ ë¹„ìœ¨ (íŒŒì´ì°¨íŠ¸)
# --------------------------------------------

# 1. êµ°ì§‘ë³„-ë¶„ê¸°ë³„ ì—…ì†Œìˆ˜ í•©ê³„ì™€ ì í¬ìˆ˜ í•©ê³„ ì§‘ê³„
df_grouped = (
    df_final_cluster
    .groupby(['hier_cluster_label', 'ê¸°ì¤€_ë…„ë¶„ê¸°_ì½”ë“œ'])[['ì—…ì†Œìˆ˜', 'ì í¬_ìˆ˜']]
    .sum()
    .reset_index()
)

# 2. ë¹„ì¤‘ ê³„ì‚°
df_grouped['ì—…ì†Œìˆ˜_ë¹„ì¤‘'] = df_grouped['ì—…ì†Œìˆ˜'] / df_grouped['ì í¬_ìˆ˜']
df_grouped['ê¸°ì¤€_ë…„ë¶„ê¸°_ì½”ë“œ'] = df_grouped['ê¸°ì¤€_ë…„ë¶„ê¸°_ì½”ë“œ'].astype('str')


# 3. Plotly ë¼ì¸ì°¨íŠ¸ ì‹œê°í™”
fig = px.line(
    df_grouped,
    x='ê¸°ì¤€_ë…„ë¶„ê¸°_ì½”ë“œ',
    y='ì—…ì†Œìˆ˜_ë¹„ì¤‘',
    color='hier_cluster_label',
    markers=True,
    title='í´ëŸ¬ìŠ¤í„°ë³„ ì í¬ìˆ˜ ëŒ€ë¹„ ì°©í•œê°€ê²© ì—…ì†Œ ë¹„ì¤‘ ì¶”ì´',
    labels={
        'ê¸°ì¤€_ë…„ë¶„ê¸°_ì½”ë“œ': 'ê¸°ì¤€ ë…„ë¶„ê¸°',
        'ì—…ì†Œìˆ˜_ë¹„ì¤‘': 'ì—…ì†Œìˆ˜ ë¹„ì¤‘',
        'hier_cluster_label': 'í´ëŸ¬ìŠ¤í„°'
    }
)

fig.update_layout(template='plotly_white')
fig.show()

# ------------------------------------------------------------------
#   [Part2. ì§€ë„ ì‹œê°í™”]
#   - ì‹œê°í™”1. ì°©í•œê°€ê²©ì—…ì†Œ ë¶„í¬ ì  ì‹œê°í™” (mapbox)
#   - ì‹œê°í™”2. í´ëŸ¬ìŠ¤í„°ë§(k-means, 4ê°œ) - ì°©í•œê°€ê²©ì—…ì†Œìˆ˜ ì‹œê°í™” (mapbox)
# ------------------------------------------------------------------
# ------------------------------------------------------------------
# ì‹œê°í™”1. ì°©í•œê°€ê²©ì—…ì†Œ ë¶„í¬ ì  ì‹œê°í™” (mapbox)
# -----------------------------------------------------------------
# ------------------------------------------------------------------
# ì‹œê°í™”2. í´ëŸ¬ìŠ¤í„°ë§(ê³„ì¸µì  í´ëŸ¬ìŠ¤í„°ë§, 4ê°œ) - ì°©í•œê°€ê²©ì—…ì†Œìˆ˜ ì‹œê°í™” (mapbox) 
# ------------------------------------------------------------------
df_final_cluster = pd.read_csv('./model/final_cluster.csv', encoding='utf-8')
df_final_cluster_20244 = df_final_cluster[df_final_cluster['ê¸°ì¤€_ë…„ë¶„ê¸°_ì½”ë“œ']==20244]

# í–‰ì •êµ¬ì—­ ê²½ê³„
geometry = gpd.read_file('./data/sig.shp', encoding="utf8")
geometry.rename(columns={'ADSTRD_CD': 'í–‰ì •ë™_ì½”ë“œ'}, inplace=True)
geometry['í–‰ì •ë™_ì½”ë“œ']= geometry['í–‰ì •ë™_ì½”ë“œ'].astype('str')
df_final_cluster_20244['í–‰ì •ë™_ì½”ë“œ']= df_final_cluster_20244['í–‰ì •ë™_ì½”ë“œ'].astype('str')

# ë³‘í•©
merged = df_final_cluster_20244.merge(geometry, on= 'í–‰ì •ë™_ì½”ë“œ')

# GeoDataFrameìœ¼ë¡œ ë³€í™˜
gdf = gpd.GeoDataFrame(merged, geometry='geometry')

gdf.head()
gdf = gdf.set_crs(epsg=5181)
gdf = gdf.to_crs(epsg=4326)

gdf.info()

# í´ëŸ¬ìŠ¤í„°ë§ ê¸°ì¤€ 
color_map = {
    1: [255, 0, 0],       # ì „ì—°ë ¹_ê·¹_ì €ì†Œë¹„ì§€ì—­ â†’ ë¹¨ê°•
    2: [255, 165, 0],     # ì „ì—°ë ¹_ì†Œë¹„_ë¹„í™œì„±ì§€ì—­ â†’ ì£¼í™©
    3: [0, 128, 0],       # ì¤‘ì¥ë…„_ê³ ì†Œë¹„ì§€ì—­ â†’ ì´ˆë¡
    4: [0, 0, 255],       # ì²­ë…„_ê³ ì†Œë¹„ì§€ì—­ â†’ íŒŒë‘
}

gdf[['r', 'g', 'b']] = pd.DataFrame(
    gdf['hier_cluster'].map(color_map).tolist(),
    index=gdf.index
)

# í•µì‹¬ ìˆ˜ì •ì‚¬í•­: geometryë¥¼ ì¢Œí‘œ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
def polygon_to_coords(geom):
    """Polygon geometryë¥¼ pydeckì´ ì´í•´í•  ìˆ˜ ìˆëŠ” ì¢Œí‘œ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜"""
    if geom.geom_type == 'Polygon':
        # ì™¸ë¶€ ë§ì˜ ì¢Œí‘œë§Œ ì‚¬ìš© (ë‚´ë¶€ êµ¬ë©ì€ ë¬´ì‹œ)
        return list(geom.exterior.coords)
    elif geom.geom_type == 'MultiPolygon':
        # MultiPolygonì¸ ê²½ìš° ì²« ë²ˆì§¸ í´ë¦¬ê³¤ë§Œ ì‚¬ìš©
        return list(geom.geoms[0].exterior.coords)
    else:
        return []

# geometryë¥¼ ì¢Œí‘œ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
gdf['coordinates'] = gdf['geometry'].apply(polygon_to_coords)

# í–‰ì •ë™ ì¤‘ì‹¬ì  ê³„ì‚° (ë¼ë²¨ë§ìš©)
gdf['centroid'] = gdf['geometry'].centroid
gdf['lon'] = gdf['centroid'].x
gdf['lat'] = gdf['centroid'].y

# í–‰ì •ë™ëª… ë§¤í•‘
# 1. JSON ë§¤í•‘ íŒŒì¼ ë¶ˆëŸ¬ì˜¤ê¸°
with open('./util/map.json', encoding='utf-8') as f:
    dong_map = json.load(f)

# 2. 'í–‰ì •ë™' ì»¬ëŸ¼ì„ ê¸°ë°˜ìœ¼ë¡œ ì˜ë¬¸ëª…ì„ ë§¤í•‘í•˜ì—¬ ìƒˆ ì»¬ëŸ¼ ìƒì„±
gdf['í–‰ì •ë™_ì˜ë¬¸'] = gdf['í–‰ì •ë™'].map(dong_map)

# 3. ë§¤í•‘ì´ ì•ˆ ëœ ê²½ìš° í™•ì¸ (ì„ íƒì )
unmapped = gdf[gdf['í–‰ì •ë™_ì˜ë¬¸'].isna()]['í–‰ì •ë™'].unique()
if len(unmapped) > 0:
    print("ë‹¤ìŒ í–‰ì •ë™ì€ ë§¤í•‘ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤:")
    print(unmapped)

# DataFrameìœ¼ë¡œ ë³€í™˜ (geometry ì»¬ëŸ¼ ì œì™¸)
df_for_pydeck = gdf.drop(columns=['geometry','centroid']).copy()
df_for_pydeck = pd.DataFrame(df_for_pydeck)

# PolygonLayer ìƒì„±
polygon_layer = pdk.Layer(
    "PolygonLayer",
    df_for_pydeck,
    get_polygon='coordinates',  # ë³€í™˜ëœ ì¢Œí‘œ ì‚¬ìš©
    get_fill_color=['r', 'g', 'b'],  # í´ëŸ¬ìŠ¤í„°ë§ì— ë”°ë¥¸ ìƒ‰ìƒ
    get_elevation='ì°©í•œê°€ê²©_ì—…ì†Œìˆ˜_ë¹„ì¤‘',
    elevation_scale=10000,
    extruded=True,  
    pickable=True,
    auto_highlight=True,
    get_line_color=[255, 255, 255],  # ê²½ê³„ì„  ìƒ‰ìƒ (í°ìƒ‰)
    line_width_min_pixels=1,
)


# TextLayer ìƒì„± (í–‰ì •ë™ ë¼ë²¨)
text_layer = pdk.Layer(
    "TextLayer",
    df_for_pydeck,
    get_position=['lon', 'lat',1500],
    get_text='í–‰ì •ë™_ì˜ë¬¸',
    get_size=5,
    get_color=[255, 255, 255, 255],  # í°ìƒ‰ í…ìŠ¤íŠ¸
    get_angle=0,
    pickable=False,
    billboard=True,
    stroked=True
)

# ì„œìš¸ ì¤‘ì‹¬ ì¢Œí‘œ
seoul_center = [126.9780, 37.5665]
view_state = pdk.ViewState(
    longitude=seoul_center[0],
    latitude=seoul_center[1],
    zoom=10,
    pitch=45,
    bearing=0
)

r = pdk.Deck(
    layers=[polygon_layer, text_layer],
    initial_view_state=view_state,
    tooltip={"text": "í–‰ì •ë™: {í–‰ì •ë™}\nì°©í•œê°€ê²©ì—…ì†Œ ë¹„ì¤‘: {ì°©í•œê°€ê²©_ì—…ì†Œìˆ˜_ë¹„ì¤‘}"},
)

def create_map_with_legend(deck_obj, filename="map_with_legend.html"):
    # HTML ë¬¸ìì—´ë¡œ ë°›ì•„ì˜¤ê¸° (ì¤‘ìš”: as_string=True ì¶”ê°€)
    original_html = deck_obj.to_html(as_string=True)

    legend_html = """
    <div class="legend" style="
        position: absolute;
        top: 20px;
        right: 20px;
        background: rgba(20, 20, 20, 0.95);
        padding: 20px;
        border-radius: 12px;
        border: 2px solid #444;
        font-size: 13px;
        z-index: 1000;
        width: 230px;
        font-family: 'Segoe UI', Arial, sans-serif;
        box-shadow: 0 8px 32px rgba(0, 0, 0, 0.6);
        backdrop-filter: blur(10px);
    ">
        <h3 style="
            margin: 0 0 18px 0;
            font-size: 18px;
            color: #fff;
            text-align: center;
            border-bottom: 2px solid #555;
            padding-bottom: 12px;
            font-weight: 600;
            letter-spacing: 0.5px;
        ">ğŸ—ºï¸ êµ°ì§‘ë³„ ìƒê¶Œ ìœ í˜•</h3>

        <!-- êµ°ì§‘ 1: ì „ì—°ë ¹_ê·¹_ì €ì†Œë¹„ì§€ì—­ -->
        <div style="display: flex; align-items: center; margin: 8px 0;">
            <div style="width: 16px; height: 12px; margin-right: 8px; border-radius: 2px; background-color: rgb(255, 0, 0);"></div>
            <span style="color: #ddd; font-size: 12px;">êµ°ì§‘1: ì „ì—°ë ¹_ê·¹_ì €ì†Œë¹„ì§€ì—­</span>
        </div>

        <!-- êµ°ì§‘ 2: ì „ì—°ë ¹_ì†Œë¹„_ë¹„í™œì„±ì§€ì—­ -->
        <div style="display: flex; align-items: center; margin: 8px 0;">
            <div style="width: 16px; height: 12px; margin-right: 8px; border-radius: 2px; background-color: rgb(255, 165, 0);"></div>
            <span style="color: #ddd; font-size: 12px;">êµ°ì§‘2: ì „ì—°ë ¹_ì†Œë¹„_ë¹„í™œì„±ì§€ì—­</span>
        </div>

        <!-- êµ°ì§‘ 3: ì¤‘ì¥ë…„_ê³ ì†Œë¹„ì§€ì—­ -->
        <div style="display: flex; align-items: center; margin: 8px 0;">
            <div style="width: 16px; height: 12px; margin-right: 8px; border-radius: 2px; background-color: rgb(0, 128, 0);"></div>
            <span style="color: #ddd; font-size: 12px;">êµ°ì§‘3: ì¤‘ì¥ë…„_ê³ ì†Œë¹„ì§€ì—­</span>
        </div>

        <!-- êµ°ì§‘ 4: ì²­ë…„_ê³ ì†Œë¹„ì§€ì—­ -->
        <div style="display: flex; align-items: center; margin: 8px 0;">
            <div style="width: 16px; height: 12px; margin-right: 8px; border-radius: 2px; background-color: rgb(0, 0, 255);"></div>
            <span style="color: #ddd; font-size: 12px;">êµ°ì§‘4: ì²­ë…„_ê³ ì†Œë¹„ì§€ì—­</span>
        </div>

        <div style="
            margin-top: 12px;
            padding: 8px;
            background: rgba(255, 255, 255, 0.05);
            border-radius: 6px;
            border: 1px solid #333;
        ">
            <div style="color: #aaa; font-size: 10px; line-height: 1.3;">
                <strong>ë¶„ì„ ê¸°ì¤€:</strong><br>
                'ì™¸ì‹ì§€ì¶œ', 'íì—…ë¥ ', '20_30_ì¸êµ¬ë¹„', 'ìƒê¶Œì¶•ì†Œì§€ì—­ì—¬ë¶€'<br>
                ë“± ì£¼ìš” íŠ¹ì„±ì„ ê¸°ë°˜ìœ¼ë¡œ 4ê°œ êµ°ì§‘ìœ¼ë¡œ ë¶„ë¥˜
            </div>
        </div>

        <div style="
            margin-top: 8px;
            text-align: center;
            color: #666;
            font-size: 10px;
            border-top: 1px solid #333;
            padding-top: 6px;
        ">
            ì§€ë„ë¥¼ í´ë¦­í•˜ë©´ í•´ë‹¹ ì§€ì—­ì˜ ì„¸ë¶€ ì •ë³´ë¥¼ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
        </div>
    </div>
    """

    body_style = """
    <style>
        body {
            background: #1a1a1a !important;
        }
        .legend:hover {
            transform: translateY(-2px);
            transition: transform 0.2s ease;
        }
    </style>
    """

    # ìˆ˜ì •ëœ HTML ì‚½ì…
    modified_html = original_html.replace('</body>', legend_html + '\n</body>')
    modified_html = modified_html.replace('</head>', body_style + '\n</head>')

    with open(filename, "w", encoding="utf-8") as f:
        f.write(modified_html)

    print(f"ğŸ“ ê°œì„ ëœ ë²”ë¡€ê°€ í¬í•¨ëœ ì§€ë„ê°€ '{filename}'ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    return filename


"""
# ë¶„ê¸° ì½”ë“œ â†’ ë‚ ì§œí˜•ìœ¼ë¡œ ë³€í™˜ í•¨ìˆ˜
def convert_quarter_to_date(code):
    year = int(str(code)[:4])
    quarter = int(str(code)[-1])
    month = (quarter - 1) * 3 + 1
    return pd.to_datetime(f"{year}-{month:02d}-01")

# Prophetì„ í†µí•œ ì¶”ì„¸ ë¶„ì„ í•¨ìˆ˜
def trend_analysis(df : pd.DataFrame, target_col : str):
    results = []

    # tqdmìœ¼ë¡œ ì§„í–‰ë¥  í™•ì¸
    for adstrd_cd, group in tqdm(df.groupby('í–‰ì •ë™_ì½”ë“œ')):
        group = group.sort_values('ê¸°ì¤€_ë…„ë¶„ê¸°_ì½”ë“œ')
        group['ds'] = group['ê¸°ì¤€_ë…„ë¶„ê¸°_ì½”ë“œ'].apply(convert_quarter_to_date)
        group['y'] = group[target_col]

        if len(group) < 4:  # ìµœì†Œ ë¶„ê¸° ìˆ˜ ì œí•œ
            continue

        try:
            model = Prophet(
                yearly_seasonality=False,
                weekly_seasonality=False,
                daily_seasonality=False
            )
            model.fit(group[['ds', 'y']])
            forecast = model.predict(group[['ds']])

            # ì¶”ì„¸(trend), time(ë¶„ê¸°) ì¶”ì¶œ
            trend = forecast['trend'].values
            time = group['ds'].map(pd.Timestamp.toordinal).values

            # ê¸°ìš¸ê¸° ê³„ì‚°(ìµœì†Œì œê³±ë²•ìœ¼ë¡œ ì§ì„ ì˜ ê¸°ìš¸ê¸° ê³„ì‚°)
            slope = ((trend - trend.mean()) * (time - time.mean())).sum() / ((time - time.mean())**2).sum()

            results.append({
                'í–‰ì •ë™_ì½”ë“œ': adstrd_cd,
                'ì¶”ì„¸ê¸°ìš¸ê¸°': slope,
                'trend_min': trend.min(),
                'trend_max': trend.max(),
                'trend_diff': trend.max() - trend.min(),
            })

        except Exception as e:
            print(f"{adstrd_cd} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

    return results


sales_slope = trend_analysis(sales_grouped, 'ë‹¹ì›”_ë§¤ì¶œ_ê¸ˆì•¡')
markets_slope = trend_analysis(market_grouped, 'ì í¬_ìˆ˜')

df_sales_slope = pd.DataFrame(sales_slope)
df_sales_slope.rename(columns={'ì¶”ì„¸ê¸°ìš¸ê¸°': 'sales_slope'}, inplace=True)

df_markets_slope = pd.DataFrame(markets_slope)
df_markets_slope.rename(columns={'ì¶”ì„¸ê¸°ìš¸ê¸°': 'markets_slope'}, inplace=True)

geometry.rename(columns={'ADSTRD_CD': 'í–‰ì •ë™_ì½”ë“œ'}, inplace=True)
geometry['í–‰ì •ë™_ì½”ë“œ']= geometry['í–‰ì •ë™_ì½”ë“œ'].astype('str')

df_sales_slope['í–‰ì •ë™_ì½”ë“œ']= df_sales_slope['í–‰ì •ë™_ì½”ë“œ'].astype('str')
df_markets_slope['í–‰ì •ë™_ì½”ë“œ']= df_markets_slope['í–‰ì •ë™_ì½”ë“œ'].astype('str')

merged = geometry.merge(df_sales_slope, on= 'í–‰ì •ë™_ì½”ë“œ')
merged = merged.merge(df_markets_slope, on= 'í–‰ì •ë™_ì½”ë“œ')

def classify(row):
    if row['sales_slope'] > 0 and row['markets_slope'] >= 0:
        return 1  # Class 1
    elif row['sales_slope'] < 0 and row['markets_slope'] > 0:
        return 2  # Class 2
    elif row['sales_slope'] < 0 and row['markets_slope'] <= 0:
        return 3  # Class 3
    else:
        return 4  # Class 4

def calc_interaction_weight(row):
     return row['sales_norm'] * np.exp(row['markets_norm']-1)

def prepcs_derived_feature(df_base : pd.DataFrame):
    # interact_growth ê³„ì‚°
    scaler1 = MinMaxScaler(feature_range=(1000, 10000))
    scaler2 = MinMaxScaler()
    df_base['sales_norm'] = scaler1.fit_transform(np.abs(df_base[['sales_slope']]))
    df_base['markets_norm'] = scaler2.fit_transform(np.abs(df_base[['markets_slope']]))

    return df_base

# í´ë˜ìŠ¤ ë¶„ë¥˜ 
merged['class'] = merged.apply(classify, axis=1)
merged = prepcs_derived_feature(merged)
merged['interact_growth'] = merged.apply(calc_interaction_weight, axis=1)



def create_auto_color_stops(series, n_bins=6, round_base=100):
 
    #Pandas Seriesë¡œë¶€í„° ìë™ numeric_stops ìƒì„±

    #Parameters:
    #- series: ìˆ«ìê°’ Series (ì˜ˆ: df['interact_growth'])
    #- n_bins: êµ¬ê°„ ê°œìˆ˜
    #- round_base: êµ¬ê°„ì„ ëª‡ ë‹¨ìœ„ë¡œ ë°˜ì˜¬ë¦¼í• ì§€

    #Returns:
    #- create_numeric_stops()ì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” stops ë¦¬ìŠ¤íŠ¸

    min_val = series.min()
    max_val = series.max()

    # linspaceë¡œ êµ¬ê°„ ìƒì„± â†’ ì •ìˆ˜ ë°˜ì˜¬ë¦¼
    raw_stops = np.linspace(min_val, max_val, n_bins)
    rounded_stops = [round(x / round_base) * round_base for x in raw_stops]

    # ì¤‘ë³µ ì œê±° ë° ì •ë ¬
    final_stops = sorted(set(rounded_stops))

    return create_color_stops(final_stops, colors='BuPu')

# í´ë˜ìŠ¤ë³„ ë°ì´í„°í”„ë ˆì„ ìƒì„±
merged_class1 = merged[merged['class'] ==1]
merged_class2 = merged[merged['class'] ==2]
merged_class3 = merged[merged['class'] ==3]
merged_class4 = merged[merged['class'] ==4]

merged_class1_subset = merged_class1[['geometry','í–‰ì •ë™_ì½”ë“œ', 'ADSTRD_NM','sales_slope','markets_slope','sales_norm','markets_norm','interact_growth']]
gdf_class1 = gpd.GeoDataFrame(merged_class1_subset, geometry='geometry')

gdf_class1 = gdf_class1.set_crs(epsg=5181)
gdf_class1 = gdf_class1.to_crs(epsg=4326)

gdf_class1.to_file('class1-geoj.geojson', driver="GeoJSON")
with open('class1-geoj.geojson', 'rt', encoding='utf-8') as f:
    gj_class1 = geojson.load(f)

# í´ë˜ìŠ¤ ì»¬ëŸ¬ stops
class_color_stops = create_auto_color_stops(gdf_class1['interact_growth'])
# ë†’ì´ ì‹œê°í™”ì„¤ì • 
numeric_stops = create_auto_numeric_stops(gdf_class1['interact_growth'])


merged_class2_subset = merged_class2[['geometry','í–‰ì •ë™_ì½”ë“œ', 'ADSTRD_NM','sales_slope','markets_slope','sales_norm','markets_norm','interact_growth']]
gdf_class2 = gpd.GeoDataFrame(merged_class2_subset, geometry='geometry')

gdf_class2 = gdf_class2.set_crs(epsg=5181)
gdf_class2 = gdf_class2.to_crs(epsg=4326)

gdf_class2.to_file('class2-geoj.geojson', driver="GeoJSON")
with open('class2-geoj.geojson', 'rt', encoding='utf-8') as f:
    gj_class2 = geojson.load(f)

# í´ë˜ìŠ¤ ì»¬ëŸ¬ stops
class_color_stops = create_auto_color_stops(gdf_class2['interact_growth'])
# ë†’ì´ ì‹œê°í™”ì„¤ì • 
numeric_stops = create_auto_numeric_stops(gdf_class2['interact_growth'])

# Choropleth ì‹œê°í™” ê°ì²´ ìƒì„±
viz = ChoroplethViz(
    access_token=token,
    data=gj_class1,
    color_property='interact_growth',
    color_stops=class_color_stops,
    center=seoul_center,
    zoom=10)

# ì„±ì¥ì„¸(ë§¤ì¶œì¶”ì„¸*ì í¬ì¶”ì„¸ì˜ interaction ì„ ë†’ì´ë¡œ ì„¤ì •)
viz.bearing = -15
viz.pitch = 45

viz.height_property = 'interact_growth'
viz.height_stops = numeric_stops
viz.height_function_type = 'interpolate'

viz.show()
"""

gdf = load_clustered_geodataframe()

# ë°ì´í„° ë””ë²„ê¹… ì½”ë“œ
print("=== GeoDataFrame ë””ë²„ê¹… ===")
print(f"GDF íƒ€ì…: {type(gdf)}")
print(f"ì „ì²´ í–‰ ìˆ˜: {len(gdf)}")
print(f"ì»¬ëŸ¼ë“¤: {list(gdf.columns)}")

print("\n=== geometry ì»¬ëŸ¼ ë¶„ì„ ===")
print(f"geometry íƒ€ì…: {type(gdf['geometry'])}")
print(f"geometry dtype: {gdf['geometry'].dtype}")
print(f"NaN ê°œìˆ˜: {gdf['geometry'].isna().sum()}")
print(f"ì²« ë²ˆì§¸ geometry: {gdf['geometry'].iloc[0] if len(gdf) > 0 else 'No data'}")

print("\n=== í•„ìˆ˜ ì»¬ëŸ¼ í™•ì¸ ===")
required_cols = ['kmeans_cluster', 'ì°©í•œê°€ê²©_ì—…ì†Œìˆ˜_ë¹„ì¤‘', 'í–‰ì •ë™']
for col in required_cols:
    if col in gdf.columns:
        print(f"{col}: OK (íƒ€ì…: {gdf[col].dtype})")
        print(f"  - NaN ê°œìˆ˜: {gdf[col].isna().sum()}")
        if col == 'kmeans_cluster':
            print(f"  - ê³ ìœ ê°’: {gdf[col].unique()}")
    else:
        print(f"{col}: ëˆ„ë½!")

# geometryê°€ ì‹¤ì œë¡œ ì§€ì˜¤ë©”íŠ¸ë¦¬ì¸ì§€ í™•ì¸
if len(gdf) > 0 and not gdf['geometry'].isna().iloc[0]:
    try:
        first_geom = gdf['geometry'].iloc[0]
        print(f"\nì²« ë²ˆì§¸ geometry ì†ì„±:")
        print(f"  - geom_type: {first_geom.geom_type}")
        print(f"  - is_valid: {first_geom.is_valid}")
        print(f"  - bounds: {first_geom.bounds}")
    except Exception as e:
        print(f"Geometry ê²€ì¦ ì¤‘ ì˜¤ë¥˜: {e}")