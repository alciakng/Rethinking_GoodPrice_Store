import pandas as pd
import geopandas as gpd
import numpy as np

from sklearn.metrics import mean_squared_log_error

from scipy.stats import zscore
from scipy.stats import skew

import seaborn as sns
import matplotlib as mpl
import matplotlib.pyplot as plt

# =======================================
# 0. ê³µí†µí•¨ìˆ˜_ì •ì˜
# - zscore_scale 
# - check_variable_skewness (ì™œë„íŒŒì•…)
# - check_outliers_std (ì´ìƒì¹˜íŒŒì•…)
# =======================================

# ë¡œê·¸ë³€í™˜ í•¨ìˆ˜
def apply_log_transform(df, columns):
    """
    ë¡œê·¸ ë˜ëŠ” log1p ë³€í™˜ì„ ìˆ˜í–‰í•˜ê³ , ì›ë³¸ ì»¬ëŸ¼ëª… ë’¤ì— '_log'ë¥¼ ë¶™ì—¬ ìƒˆë¡œìš´ ì»¬ëŸ¼ ìƒì„±
    :param df: ì›ë³¸ DataFrame
    :param columns: ë¡œê·¸ ë³€í™˜í•  ì»¬ëŸ¼ ë¦¬ìŠ¤íŠ¸
    :return: ë³€í™˜ëœ DataFrame (in-place ì•„ë‹˜)
    """
    df_transformed = df.copy()
    
    for col in columns:
        if (df_transformed[col] <= 0).any():
            print(col + " â†’ log1p ì ìš©")
            df_transformed[f'{col}_log'] = np.log1p(df_transformed[col])
        else:
            print(col + " â†’ log ì ìš©")
            df_transformed[f'{col}_log'] = np.log(df_transformed[col])
    
    return df_transformed


# zscore_scale
def apply_zscore_scaling(df, columns):
    """
    ì§€ì •ëœ ì»¬ëŸ¼ì— ëŒ€í•´ Z-score ìŠ¤ì¼€ì¼ë§ì„ ìˆ˜í–‰
    :param df: ì›ë³¸ DataFrame
    :param columns: ìŠ¤ì¼€ì¼ë§í•  ì»¬ëŸ¼ ë¦¬ìŠ¤íŠ¸
    :return: ìŠ¤ì¼€ì¼ë§ëœ DataFrame (in-place ì•„ë‹˜)
    """
    df_scaled = df.copy()
    
    for col in columns:
        print(col + " â†’ zscore scaling")
        df_scaled[col] = zscore(df_scaled[col])
    
    return df_scaled

mpl.rc('font', family='AppleGothic') # í•œê¸€ê¹¨ì§ ë¬¸ì œ
def check_variable_skewness(df, threshold=1.0):
    numeric_cols = df.select_dtypes(include=[np.number]).columns
    skew_info = {}

    for col in numeric_cols:
        col_skew = skew(df[col].dropna())
        skew_info[col] = col_skew

    skew_df = pd.DataFrame.from_dict(skew_info, orient='index', columns=['Skewness'])
    skew_df = skew_df.sort_values('Skewness', ascending=False)

    # ê¸°ì¤€ì„  í‘œì‹œ
    def skew_label(value):
        if abs(value) < 0.5:
            return 'âˆ¼ ëŒ€ì¹­'
        elif abs(value) < threshold:
            return 'ì•½ê°„ ì™œë„'
        else:
            return 'ê°•í•œ ì™œë„'

    skew_df['í•´ì„'] = skew_df['Skewness'].apply(skew_label)

    print("ğŸ“Š ë³€ìˆ˜ë³„ ì™œë„ ìš”ì•½:")
    print(skew_df)

    # íˆìŠ¤í† ê·¸ë¨ ì‹œê°í™”
    fig, axs = plt.subplots(nrows=int(np.ceil(len(numeric_cols)/3)), ncols=3, figsize=(16, int(len(numeric_cols)*1.5)))
    axs = axs.flatten()

    for i, col in enumerate(numeric_cols):
        sns.histplot(df[col].dropna(), kde=True, ax=axs[i])
        axs[i].set_title(f"{col} (Skew: {skew_info[col]:.2f})")

    plt.tight_layout()
    plt.show()

    return skew_df

def check_outliers_std(df, threshold=3.0):
    numeric_cols = df.select_dtypes(include=[np.number]).columns
    outlier_info = {}

    for col in numeric_cols:
        data = df[col].dropna()
        mean = data.mean()
        std = data.std()
        lower_bound = mean - threshold * std
        upper_bound = mean + threshold * std

        outliers = data[(data < lower_bound) | (data > upper_bound)]
        outlier_ratio = len(outliers) / len(data)

        outlier_info[col] = {
            'Outliers': len(outliers),
            'Outlier_Ratio': outlier_ratio,
            'Mean': mean,
            'Std': std,
            'Lower_Bound': lower_bound,
            'Upper_Bound': upper_bound
        }

    # ìš”ì•½í‘œ ìƒì„±
    outlier_df = pd.DataFrame(outlier_info).T
    outlier_df = outlier_df.sort_values('Outlier_Ratio', ascending=False)

    print("ğŸ“ 3Ïƒ ê¸°ì¤€ ì´ìƒì¹˜ íƒì§€ ê²°ê³¼:")
    print(outlier_df[['Outliers', 'Outlier_Ratio']])

    # ì‹œê°í™”
    fig, axs = plt.subplots(nrows=int(np.ceil(len(numeric_cols)/3)), ncols=3, figsize=(16, int(len(numeric_cols)*1.5)))
    axs = axs.flatten()

    for i, col in enumerate(numeric_cols):
        sns.histplot(df[col].dropna(), kde=True, ax=axs[i], color='lightcoral')
        axs[i].axvline(outlier_info[col]['Lower_Bound'], color='blue', linestyle='--', label='Lower Bound')
        axs[i].axvline(outlier_info[col]['Upper_Bound'], color='blue', linestyle='--', label='Upper Bound')
        axs[i].set_title(f"{col} (Outliers: {outlier_info[col]['Outliers']})")
        axs[i].legend()

    plt.tight_layout()
    plt.show()

    return outlier_df

# ì´ìƒì¹˜ì œê±° í•¨ìˆ˜ 
def drop_outlier_rows_std(df, cols, threshold=3.0):
    df_cleaned = df.copy()
    valid_cols = []

    # ìœ íš¨í•œ ìˆ˜ì¹˜í˜• ì»¬ëŸ¼ë§Œ í•„í„°ë§
    for col in cols:
        if col in df.columns and np.issubdtype(df[col].dtype, np.number):
            valid_cols.append(col)
        else:
            print(f"âš ï¸ ì»¬ëŸ¼ '{col}'ì€ ì¡´ì¬í•˜ì§€ ì•Šê±°ë‚˜ ìˆ˜ì¹˜í˜•ì´ ì•„ë‹™ë‹ˆë‹¤. ì œì™¸ë©ë‹ˆë‹¤.")

    if not valid_cols:
        print("âŒ ì´ìƒì¹˜ ê²€ì¶œí•  ìˆ˜ ìˆëŠ” ìˆ˜ì¹˜í˜• ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        return df_cleaned

    # ì´ìƒì¹˜ ë§ˆìŠ¤í¬ ìƒì„±
    outlier_mask = pd.DataFrame(False, index=df.index, columns=valid_cols)

    for col in valid_cols:
        data = df[col]
        mean = data.mean()
        std = data.std()
        lower_bound = mean - threshold * std
        upper_bound = mean + threshold * std

        outlier_mask[col] = (data < lower_bound) | (data > upper_bound)

    # ì´ìƒì¹˜ í¬í•¨ëœ í–‰ ì‹ë³„
    rows_with_outliers = outlier_mask.any(axis=1)
    num_outliers = rows_with_outliers.sum()

    print(f"ğŸ§¹ ì´ìƒì¹˜ í¬í•¨ í–‰ ì œê±°: {num_outliers}ê°œ í–‰ ì‚­ì œë¨")

    # ì¸ë±ìŠ¤ ìœ ì§€í•œ ì±„ ì´ìƒì¹˜ í–‰ ì œê±° (reset_index ì œê±°)
    df_no_outliers = df_cleaned[~rows_with_outliers]

    return df_no_outliers

# rmsle ê³„ì‚°í•¨ìˆ˜ 
def compute_rmsle_from_result(result, df):
    """
    PanelOLS íšŒê·€ ê²°ê³¼ì—ì„œ RMSLEë¥¼ ê³„ì‚°í•˜ëŠ” í•¨ìˆ˜
    
    Parameters:
    -----------
    result : linearmodels.panel.results.PanelEffectsResults
        PanelOLSì˜ íšŒê·€ ê²°ê³¼ ê°ì²´ (ex: result = model.fit())
    
    df : pd.DataFrame
        ì›ë³¸ ë°ì´í„°í”„ë ˆì„ (fitted_valuesì˜ ì¸ë±ìŠ¤ì™€ ë§ì•„ì•¼ í•¨)
    
    Returns:
    --------
    rmsle : float
        ë¡œê·¸ ì—­ë³€í™˜ í›„ ì˜ˆì¸¡ê°’ê³¼ ì‹¤ì œê°’ ê°„ RMSLE
    """
    # ì¢…ì†ë³€ìˆ˜ëª… ì¶”ì¶œ
    y_var = result.model.dependent.vars[0]

    # ì˜ˆì¸¡ê°’ (log scale)
    y_pred_log = result.fitted_values

    # ì‹¤ì œê°’ (log scale)
    y_true_log = df.loc[y_pred_log.index, y_var]

    # ë¡œê·¸ ì—­ë³€í™˜
    y_pred_actual = np.expm1(y_pred_log)
    y_true_actual = np.expm1(y_true_log)

    # ìŒìˆ˜ ë°©ì§€ (RMSLEëŠ” ìŒìˆ˜ ì…ë ¥ ë¶ˆê°€)
    y_pred_fixed = np.clip(y_pred_actual, 0, None)
    y_true_fixed = np.clip(y_true_actual, 0, None)

    # RMSLE ê³„ì‚°
    rmsle = np.sqrt(mean_squared_log_error(y_true_fixed, y_pred_fixed))
    return rmsle


def save_full_model_output(results, rmsle=None, filename="full_model_output.csv"):
    # --- ìš”ì•½ í†µê³„ ---
    n_obs = results.nobs   
    n_periods = getattr(results, 'time_info', {}).get('total', None) 
    k = len(results.params)
    r2 = results.rsquared
    adj_r2 = 1 - (1 - r2) * (n_obs - 1) / (n_obs - k - 1)

    summary_dict = {
        "No. of Observations": n_obs,
        "No. of Time Periods": n_periods,
        "R-squared (Overall)": round(r2, 4),
        "Adjusted R-squared": round(adj_r2, 4),
        "R-squared (Within)": round(results.rsquared_within, 4),
        "R-squared (Between)": round(results.rsquared_between, 4),
        "Log-likelihood": round(results.loglik, 2),
        "F-statistic": round(results.f_statistic.stat, 2),
        "F-statistic (p-value)": round(results.f_statistic.pval, 4),
        "RMSLE": round(rmsle, 4) if rmsle is not None else "N/A"
    }

    df_summary = pd.DataFrame(summary_dict.items(), columns=["Metric", "Value"])

    # --- ê³„ìˆ˜ í…Œì´ë¸” ---
    df_coef = pd.DataFrame({
        'Variable': results.params.index,
        'Coef.': results.params.values,
        'Std.Err.': results.std_errors.values,
        'T-Stat': results.tstats.values,
        'P-Value': results.pvalues.values,
        'CI Lower': results.conf_int().iloc[:, 0].values,
        'CI Upper': results.conf_int().iloc[:, 1].values,
    }).round(4)

    # --- êµ¬ë¶„ì„  ---
    separator = pd.DataFrame([["---", "---"]], columns=df_summary.columns)

    # --- í•©ì¹˜ê¸° ---
    df_combined = pd.concat([df_summary, separator, df_coef], ignore_index=True)

    # ì €ì¥
    df_combined.to_csv(filename, index=False, encoding='utf-8-sig')


def load_model_result(csv_path):
    df = pd.read_csv(csv_path)
    separator_idx = df[df.iloc[:, 0] == "---"].index[0]

    df_summary = df.iloc[:separator_idx].copy().reset_index(drop=True)
    df_coef = df.iloc[separator_idx+1:].copy().reset_index(drop=True)

    return df_summary, df_coef

def safe_format(x):
    try:
        return f"{float(x):.4f}"
    except:
        return x
    
def load_clustered_geodataframe(
    cluster_csv_path='./model/final_cluster.csv',
    shapefile_path='./data/sig.shp',
    target_quarter=20244
):
    """
    í´ëŸ¬ìŠ¤í„°ë§ ê²°ê³¼ì™€ í–‰ì •ë™ ê²½ê³„ ë°ì´í„°ë¥¼ ë³‘í•©í•˜ì—¬ GeoDataFrameì„ ìƒì„±í•©ë‹ˆë‹¤.

    Parameters:
    - cluster_csv_path (str): í´ëŸ¬ìŠ¤í„° ê²°ê³¼ CSV íŒŒì¼ ê²½ë¡œ
    - shapefile_path (str): í–‰ì •ë™ ê²½ê³„ Shapefile ê²½ë¡œ
    - target_quarter (int): í•„í„°ë§í•  ê¸°ì¤€ ì—°ë¶„ê¸° ì½”ë“œ (ì˜ˆ: 20244)

    Returns:
    - gdf (GeoDataFrame): í´ëŸ¬ìŠ¤í„°ë§ ì •ë³´ê°€ í¬í•¨ëœ ê³µê°„ ë°ì´í„°
    """

    df_final_cluster = pd.read_csv(cluster_csv_path, encoding='utf-8')
    df_final_cluster_20244 = df_final_cluster[df_final_cluster['ê¸°ì¤€_ë…„ë¶„ê¸°_ì½”ë“œ']==target_quarter]

    # í–‰ì •êµ¬ì—­ ê²½ê³„
    geometry = gpd.read_file(shapefile_path, encoding="utf8")
    geometry.rename(columns={'ADSTRD_CD': 'í–‰ì •ë™_ì½”ë“œ'}, inplace=True)
    geometry['í–‰ì •ë™_ì½”ë“œ']= geometry['í–‰ì •ë™_ì½”ë“œ'].astype('str')
    df_final_cluster_20244['í–‰ì •ë™_ì½”ë“œ']= df_final_cluster_20244['í–‰ì •ë™_ì½”ë“œ'].astype('str')

    # ë³‘í•©
    merged = df_final_cluster_20244.merge(geometry, on= 'í–‰ì •ë™_ì½”ë“œ')

    # GeoDataFrameìœ¼ë¡œ ë³€í™˜
    gdf = gpd.GeoDataFrame(merged, geometry='geometry')
    gdf = gdf.set_crs(epsg=5181)
    gdf = gdf.to_crs(epsg=4326)

    return gdf
